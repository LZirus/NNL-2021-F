{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing opencv haarcascade face detection network\n",
    "https://github.com/opencv/opencv/tree/master/data/haarcascades\n",
    "explained in: https://www.youtube.com/watch?v=7IFhsbfby9s&t=300s (or gitHub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertImg(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def faceNetLocalize(img):\n",
    "    img_cvt = convertImg(img)\n",
    "    return faceNet.detectMultiScale3(img_cvt, scaleFactor=scaleFactor, minNeighbors=minNeighbors, minSize=minSize, outputRejectLevels = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaleFactor = 1.1 #between 1.05 (quality) and 1.4 (speed) recommended (scale of the faces we search for)\n",
    "minNeighbors = 4 #between 3 (quantity) and 6 (quality) recommended\n",
    "minSize = (10, 10) #min size of a face in the picture\n",
    "#maxSize = (240, 240) #max face size in picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: testing with different model types\n",
    "#for example: eye model + larger bounding box towards the bottom\n",
    "faceNet = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "#img = cv2.imread('../img/train/with_mask/0-with-mask.jpg')\n",
    "#img = cv2.imread('testPic4.jpg')\n",
    "\n",
    "#faceLocs, rejectLevels, confidences = faceNetLocalize(img)\n",
    "#print(confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crop_imgs = []\n",
    "\n",
    "#for (x, y, w, h) in faceLocs:\n",
    "#    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "#    crop_imgs.append(img[y:y+h, x:x+w])\n",
    "    \n",
    "\n",
    "#cv2.imshow('img', img)\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing - Face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO can we even use accuracy methods? should we just test freely with live feed and empirical testing?\n",
    "#labels_path = os.path.join('..', 'img', 'test', 'labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training - Mask classifier\n",
    "\n",
    "foundation: https://www.tensorflow.org/tutorials/load_data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import MaxPool2D, Conv2D, Input, Dense, Flatten, AveragePooling2D, Dropout\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras import Sequential, losses as lfs\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_size = (180, 180)\n",
    "img_size_vgg = (224, 224)\n",
    "epochs = 10\n",
    "checkpoint_path = \"mask_model/weights.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "imgs_path = os.path.join('..', 'img')\n",
    "\n",
    "correct_usage=  'correct usage: \\n' + 'predict([path to image], \\'category\\' \\n' + 'predict([path to image], \\'probabilities\\' \\n' + 'predict([path to image], \\'detection\\' \\n' + 'predict(\\'live_detection\\')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 322 files belonging to 4 classes.\n",
      "Using 258 files for training.\n",
      "Found 322 files belonging to 4 classes.\n",
      "Using 64 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = image_dataset_from_directory(imgs_path,  validation_split=0.2, subset=\"training\",  seed=3, image_size=img_size,  batch_size=batch_size)\n",
    "val_ds = image_dataset_from_directory(imgs_path,  validation_split=0.2, subset=\"validation\",  seed=3, image_size=img_size,  batch_size=batch_size)\n",
    "labels = train_ds.class_names\n",
    "#train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "#val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "\n",
    "model = Sequential([ \n",
    "    Rescaling(1. /255),\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    AveragePooling2D(pool_size=(7,7)),\n",
    "    Flatten(name=\"flatten\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.5),#drops small confidences\n",
    "    Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "vgg_model = Sequential([\n",
    "    Rescaling(1. /255),\n",
    "    Conv2D(input_shape=(224,224,3), filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", strides=(1,1)), \n",
    "    Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2)),\n",
    "    Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2)),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2)),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2)),\n",
    "    Flatten(),\n",
    "    Dense(units=4096, activation=\"relu\"),\n",
    "    Dense(units=4096, activation=\"relu\"),\n",
    "    Dense(units=4, activation=\"softmax\")\n",
    "])\n",
    "#model = vgg_model\n",
    "\n",
    "model.compile(optimizer='adam', loss=lfs.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about the VGG-model: https://neurohive.io/en/popular-networks/vgg16/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4942: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 6s 254ms/step - loss: 1.7676 - accuracy: 0.2674 - val_loss: 1.1643 - val_accuracy: 0.4688\n",
      "\n",
      "Epoch 00001: saving model to mask_model\\weights.ckpt\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 5s 371ms/step - loss: 1.2599 - accuracy: 0.4884 - val_loss: 0.8793 - val_accuracy: 0.8281\n",
      "\n",
      "Epoch 00002: saving model to mask_model\\weights.ckpt\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 5s 349ms/step - loss: 0.8911 - accuracy: 0.6628 - val_loss: 0.5767 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00003: saving model to mask_model\\weights.ckpt\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 6s 446ms/step - loss: 0.6829 - accuracy: 0.7054 - val_loss: 0.4517 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00004: saving model to mask_model\\weights.ckpt\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 5s 367ms/step - loss: 0.6136 - accuracy: 0.7326 - val_loss: 0.4777 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00005: saving model to mask_model\\weights.ckpt\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 5s 315ms/step - loss: 0.6668 - accuracy: 0.7132 - val_loss: 0.4472 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00006: saving model to mask_model\\weights.ckpt\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 5s 311ms/step - loss: 0.5323 - accuracy: 0.7868 - val_loss: 0.4333 - val_accuracy: 0.8281\n",
      "\n",
      "Epoch 00007: saving model to mask_model\\weights.ckpt\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 4s 302ms/step - loss: 0.5031 - accuracy: 0.7984 - val_loss: 0.3193 - val_accuracy: 0.8906\n",
      "\n",
      "Epoch 00008: saving model to mask_model\\weights.ckpt\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 4s 295ms/step - loss: 0.4948 - accuracy: 0.7984 - val_loss: 0.4290 - val_accuracy: 0.8438\n",
      "\n",
      "Epoch 00009: saving model to mask_model\\weights.ckpt\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 4s 295ms/step - loss: 0.4684 - accuracy: 0.8062 - val_loss: 0.2841 - val_accuracy: 0.9219\n",
      "\n",
      "Epoch 00010: saving model to mask_model\\weights.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28d579e1310>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Callback to save model's weights\n",
    "#https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose = 1 )\n",
    "\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing (mask): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.31      0.31        13\n",
      "           1       0.42      0.43      0.43        30\n",
      "           2       0.00      0.00      0.00         4\n",
      "           3       0.38      0.35      0.36        17\n",
      "\n",
      "    accuracy                           0.36        64\n",
      "   macro avg       0.28      0.27      0.27        64\n",
      "weighted avg       0.36      0.36      0.36        64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test = np.concatenate([y for _, y in val_ds], axis=0)\n",
    "x_test = np.concatenate([x for x, _ in val_ds], axis=0)\n",
    "\n",
    "y_pred_confidences = model.predict(x_test)\n",
    "y_pred = [np.argmax(cs) for cs in y_pred_confidences]\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential([\n",
    "#    Rescaling(1. /255), \n",
    "#    Conv2D(32, 3, activation='relu'),\n",
    "#    AveragePooling2D(pool_size=(7, 7)),\n",
    "#    Flatten(name=\"flatten\"),\n",
    "#    Dense(128, activation=\"relu\"),\n",
    "#    Dropout(0.5),#drops small confidences\n",
    "#    Dense(num_classes, activation=\"softmax\")\n",
    "#    ])\n",
    "#model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskPredict(img):\n",
    "    pred = model.predict(img[None])\n",
    "    #TODO: richtiges mapping zwischen index/label?\n",
    "    label_index = np.argmax(pred)\n",
    "    return labels[label_index], pred[label_index]\n",
    "\n",
    "\n",
    "#mode can be 'category', 'probabilities', 'detection', 'live_detection'\n",
    "def predict(img_path, mode):\n",
    "    #load_img\n",
    "    img = cv2.imread(img_path)\n",
    "    cv2.imshow('img', img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    if mode=='detection':\n",
    "        detect(img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        return\n",
    "    if mode=='live_detection':\n",
    "        live_det()\n",
    "        return\n",
    "        \n",
    "    if mode=='category':\n",
    "        label, confidence = maskPredict(img)\n",
    "        return label\n",
    "        \n",
    "    if mode=='probabilities':\n",
    "        return model.predict([img])\n",
    "\n",
    "    else:\n",
    "        print(correct_usage)\n",
    "\n",
    "#def predict(mode):\n",
    "#    if mode=='live_detection':\n",
    "#        predict('', mode)\n",
    "#    else:\n",
    "#        print(correct_usage)\n",
    "\n",
    "def detect(img):\n",
    "    faceLocs, rejectLevels, confidences = faceNetLocalize(img)\n",
    "        \n",
    "    for (x, y, w, h) in faceLocs:\n",
    "        #crop image and predict label of cropped image\n",
    "        img_crop = img[y:y+h, x:x+w]\n",
    "        label, confidence_mask = maskPredict(img_crop)\n",
    "        #show label/ bounding box on image\n",
    "        cv2.putText(img, f\"{label}, confidence:{confidence_mask}\", (x+w-30, y+h), cv2.FONT_HERSHEY_PLAIN, 1.0, cv2.CV_RGB(0,255,0), 2.0) \n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow('live_output', img)\n",
    "\n",
    "def live_det():\n",
    "    #TODO: errorhandling for camera\n",
    "    \n",
    "    wait_time = 20 #time in ms to wait before refreshing feed\n",
    "    camera = cv2.Videcapture(0) #Input value might differ on different systems\n",
    "    \n",
    "    while(True):\n",
    "        _, img = camera.read()\n",
    "\n",
    "        detect(img)\n",
    "\n",
    "        #wait for ESC or q\n",
    "        if (cv2.waitKey(wait_time) & 0xFF) in [27, ord('q')]: \n",
    "            break\n",
    "\n",
    "\n",
    "    camera.release()\n",
    "    return 'live_output'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ffp2',\n",
       " array([0.47268364, 0.0024329 , 0.37747186, 0.14741163], dtype=float32))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path='../img/ffp2/IMG_1596_(657, 421, 1426, 1426).png'\n",
    "#predict(img_path, 'category')\n",
    "from tensorflow.keras.preprocessing import image\n",
    "img = image.load_img(img_path, target_size = img_size)\n",
    "img = image.img_to_array(img)\n",
    "#img = np.expand_dims(img, axis = 0)\n",
    "#np.shape(img[None])\n",
    "maskPredict(img)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "320b063615c881cf99d5bda0a05a87a093cceae5b3aed84278fa7ef3d4b95f2a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
