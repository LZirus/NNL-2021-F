{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing opencv haarcascade face detection network\n",
    "https://github.com/opencv/opencv/tree/master/data/haarcascades\n",
    "explained in: https://www.youtube.com/watch?v=7IFhsbfby9s&t=300s (or gitHub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertImg(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def faceNetLocalize(img, **kwargs):\n",
    "    scaleFactor = kwargs.get('scaleFactor', 1.1) #between 1.05 (quality) and 1.4 (speed) recommended (scale of the faces we search for)\n",
    "    minNeighbors = kwargs.get('minNeighbors', 4) #between 3 (quantity) and 6 (quality) recommended\n",
    "    minSize = kwargs.get('minSize', (10, 10)) #min size of a face in the picture\n",
    "    faceNet = kwargs.get('faceNet', init_faceNet())\n",
    "    \n",
    "    img_cvt = convertImg(img)\n",
    "    return faceNet.detectMultiScale3(img_cvt, scaleFactor=scaleFactor, minNeighbors=minNeighbors, minSize=minSize, outputRejectLevels = True)\n",
    "\n",
    "def init_faceNet(**kwargs):\n",
    "    path = kwargs.get('path', 'haarcascade_frontalface_default.xml')\n",
    "    return cv2.CascadeClassifier(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: testing with different model types\n",
    "#for example: eye model + larger bounding box towards the bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask classifier\n",
    "\n",
    "foundation: https://www.tensorflow.org/tutorials/load_data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import MaxPool2D, Conv2D, Input, Dense, Flatten, AveragePooling2D, Dropout\n",
    "import tensorflow.keras.layers as lays\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling, RandomContrast, RandomFlip, RandomRotation\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras import Sequential, losses as lfs\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation\n",
    "https://www.tensorflow.org/tutorials/images/data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = Sequential([\n",
    "  RandomFlip(\"horizontal\"),\n",
    "  RandomRotation(0.4),\n",
    "  RandomContrast(0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_size = (180, 180)\n",
    "img_size_vgg = (224, 224)\n",
    "epochs = 11\n",
    "checkpoint_path = \"mask_model/weights.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "imgs_path = os.path.join('..', 'img')\n",
    "num_classes = 3\n",
    "\n",
    "correct_usage =  'correct usage: \\n' + 'predict([path to image], \\'category\\' \\n' + 'predict([path to image], \\'probabilities\\' \\n' + 'predict([path to image], \\'detection\\' \\n' + 'predict(\\'live_detection\\')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loading dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(**kwargs):\n",
    "    imgs_path = kwargs.get('imgs_path', os.path.join('..', 'img'))\n",
    "    img_size = kwargs.get('img_size', (180, 180))\n",
    "    batch_size = kwargs.get('batch_size', 32)\n",
    "\n",
    "    ##load images/ labels from directory\n",
    "    valid_images = [\".jpg\",\".png\",\".jpeg\",\".JPG\"]\n",
    "    x=[]\n",
    "    y=[]\n",
    "    \n",
    "    for root, dirs, files in os.walk(imgs_path):\n",
    "        for filename in files:\n",
    "            end = os.path.splitext(filename)[1]\n",
    "            if end.lower() not in valid_images:\n",
    "                continue\n",
    "            image = load_img(os.path.join(root, filename), target_size=img_size)\n",
    "            image = img_to_array(image)\n",
    "            \n",
    "            label = os.path.join(root, filename).split(os.path.sep)[-2]\n",
    "            \n",
    "            x.append(image)\n",
    "            y.append(label)\n",
    "    x = np.array(x, dtype=\"float32\")\n",
    "    y = np.array(y)\n",
    "\n",
    "    ## convert labels to ints from 0 ... len(labels)-1\n",
    "    labels = []\n",
    "    for i in range(len(y)):\n",
    "        try:\n",
    "            j = labels.index(y[i])\n",
    "        except:\n",
    "            labels.append(y[i])\n",
    "            j = labels.index(y[i])\n",
    "        y[i] = j\n",
    "    y.astype(int)\n",
    "    \n",
    "    ## split dataset\n",
    "    trainX, testX, trainY, testY = train_test_split(x, y, test_size=0.2, random_state=3)\n",
    "\n",
    "    ## one-hot encoding\n",
    "    trainY = utils.to_categorical(trainY, num_classes)\n",
    "    testY = utils.to_categorical(testY, num_classes)\n",
    "\n",
    "    ## data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization=True,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    ## merge xs and ys\n",
    "    train_batches = datagen.flow(trainX, trainY, batch_size=32, subset='training')\n",
    "    test_batches  = datagen.flow(trainX, trainY, batch_size=32, subset='validation')\n",
    "    \n",
    "    return train_batches, test_batches, labels, testX, testY, trainX, trainY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypermodels\n",
    "#https://www.analyticsvidhya.com/blog/2021/06/create-convolutional-neural-network-model-and-optimize-using-keras-tuner-deep-learning/\n",
    "#https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "\n",
    "def basic_model_builder(hp):\n",
    "  \n",
    "    basic_model = Sequential([\n",
    "        Rescaling(1. /255),\n",
    "        augmentation,\n",
    "        Conv2D(filters=hp.Int('c1_filter', min_value=32, max_value=256, step=16), kernel_size=hp.Choice('c1_kernel', values=[3,5]), activation='relu'),\n",
    "        AveragePooling2D(pool_size=(7,7)),\n",
    "        Flatten(name=\"flatten\"),\n",
    "        Dense(units=hp.Int('d1_units', min_value=32, max_value=512, step=32), activation=\"relu\"),\n",
    "        Dropout(0.2),#drops small confidences\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    basic_model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), loss=lfs.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    return basic_model\n",
    "\n",
    "def small_model_builder(hp):\n",
    "  \n",
    "    small_model = Sequential([\n",
    "        Rescaling(1. /255),\n",
    "        augmentation,\n",
    "        Conv2D(filters=hp.Int('c1_filter', min_value=32, max_value=256, step=16), kernel_size=hp.Choice('c1_kernel', values=[3,5]), activation='relu'),\n",
    "        Conv2D(filters=hp.Int('c2_filter', min_value=32, max_value=256, step=16), kernel_size=hp.Choice('c2_kernel', values=[3,5]), activation='relu'),\n",
    "        MaxPool2D(pool_size=(3,3)),\n",
    "        Flatten(name=\"flatten\"),\n",
    "        Dense(units=hp.Int('d1_units', min_value=32, max_value=512, step=32), activation=\"relu\"),\n",
    "        Dropout(0.2),#drops small confidences\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    small_model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), loss=lfs.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    return small_model\n",
    "\n",
    "\n",
    "def tune_model(model_builder, xs, ys):\n",
    "    #tuner = kt.RandomSearch(model_builder, objective='val_accuracy', max_trials=5)\n",
    "    tuner = kt.RandomSearch(kt.applications.HyperResNet(input_shape=(180, 180, 3), classes=2), objective='val_loss', max_trials=5)\n",
    "    tuner = kt.Hyperband(model_builder, objective='val_accuracy', max_epochs=10, factor=3)\n",
    "    tuner.search(xs, ys, epochs=50, validation_split=0.2)\n",
    "    return tuner.get_best_hyperparameters(num_trials=1)[0], tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(model_name, **kwargs):\n",
    "    num_labels = kwargs.get('num_classes', num_classes)\n",
    "\n",
    "    basic_model = Sequential()\n",
    "    basic_model.add(Rescaling(1. /255))\n",
    "    basic_model.add(Conv2D(32, (3,3), activation='relu', input_shape=(180,180,3)))\n",
    "    basic_model.add(AveragePooling2D(pool_size=(7,7)))\n",
    "    basic_model.add(Flatten(name=\"flatten\"))\n",
    "    basic_model.add(Dense(128, activation=\"relu\"))\n",
    "    basic_model.add(Dropout(0.5)) #drops small confidences\n",
    "    basic_model.add(Dense(num_labels, activation=\"softmax\"))\n",
    "    \n",
    "\n",
    "    small_model = Sequential()\n",
    "    small_model.add(Rescaling(1. /255))\n",
    "    small_model.add(Conv2D(filters=128, kernel_size=(5,5), activation='relu', input_shape=(180,180,3)))\n",
    "    small_model.add(Conv2D(filters=128, kernel_size=(5,5), activation='relu'))\n",
    "    small_model.add(MaxPool2D(pool_size=(3,3)))\n",
    "    small_model.add(Flatten(name=\"flatten\"))\n",
    "    small_model.add(Dense(units=224, activation=\"relu\"))\n",
    "    small_model.add(Dropout(0.5))#drops small confidences\n",
    "    small_model.add(Dense(num_labels, activation=\"softmax\"))\n",
    "    \n",
    "\n",
    "    vgg_small_model=Sequential()\n",
    "\n",
    "    vgg_small_model.add(Conv2D(64,(3,3),activation='relu',input_shape=(180,180,3)))\n",
    "    vgg_small_model.add(MaxPool2D(2,2))\n",
    "    vgg_small_model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "    vgg_small_model.add(MaxPool2D(2,2))\n",
    "    vgg_small_model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "    vgg_small_model.add(MaxPool2D(2,2))\n",
    "    vgg_small_model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "    vgg_small_model.add(MaxPool2D(2,2))\n",
    "    vgg_small_model.add(Flatten())\n",
    "    vgg_small_model.add(Dropout(0.5))\n",
    "    vgg_small_model.add(Dense(120,activation='relu'))\n",
    "    vgg_small_model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "    vgg_model = Sequential()\n",
    "    vgg_model.add(Rescaling(1. /255))\n",
    "    vgg_model.add(Conv2D(input_shape=(224,224,3), filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", strides=(1,1))) \n",
    "    vgg_model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    vgg_model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(MaxPool2D(pool_size=(2, 2), strides=(2)))\n",
    "    vgg_model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(MaxPool2D(pool_size=(2, 2), strides=(2)))\n",
    "    vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(MaxPool2D(pool_size=(2, 2), strides=(2)))\n",
    "    vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(MaxPool2D(pool_size=(2, 2), strides=(2)))\n",
    "    vgg_model.add(Flatten())\n",
    "    vgg_model.add(Dense(units=4096, activation=\"relu\"))\n",
    "    vgg_model.add(Dense(units=4096, activation=\"relu\"))\n",
    "    vgg_model.add(Dense(units=num_labels, activation=\"softmax\"))\n",
    "    \n",
    "\n",
    "    if model_name == 'basic_model':\n",
    "        basic_model.summary()\n",
    "        return basic_model\n",
    "    \n",
    "    if model_name == 'small_model':\n",
    "        small_model.summary()\n",
    "        return small_model\n",
    "\n",
    "    if model_name == 'vgg_model':\n",
    "        vgg_model.summary()\n",
    "        return vgg_model\n",
    "    \n",
    "    if model_name == 'vgg_small_model':\n",
    "        vgg_small_model.summary()\n",
    "        return vgg_small_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about the VGG-model: https://neurohive.io/en/popular-networks/vgg16/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_ds, val_ds, **kwargs):\n",
    "    epochs = kwargs.get('epochs', 10)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation (mask):\n",
    "https://www.tensorflow.org/guide/keras/train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(x_test, y_test, model):\n",
    "    results = model.evaluate(x_test, y_test, batch_size=32)\n",
    "    print(results)\n",
    "    \n",
    "    y_pred_confidences = model.predict(x_test)\n",
    "    y_pred = [np.argmax(cs) for cs in y_pred_confidences]\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(**kwargs):\n",
    "    checkpoint_path = kwargs.get('checkpoint_path', \"mask_model/weights.ckpt\")\n",
    "    model = kwargs.get('model', select_model('basic_model'))\n",
    "    model.load_weights(checkpoint_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskPredict(img, labels):\n",
    "    pred = model.predict(img[None])\n",
    "    label_index = np.argmax(pred)\n",
    "    return labels[label_index], pred[0][label_index]\n",
    "\n",
    "\n",
    "#mode can be 'category', 'probabilities', 'detection', 'live_detection'\n",
    "def predict(mode, **kwargs):\n",
    "    img_path = kwargs.get('img_path', None)\n",
    "    model = kwargs.get('model', load_model())\n",
    "    if mode=='live_detection':\n",
    "        live_det()\n",
    "        return\n",
    "    \n",
    "    if img_path is None:\n",
    "        print(correct_usage)\n",
    "        return\n",
    "\n",
    "    #load_img\n",
    "    img = load_img(img_path, target_size = img_size)\n",
    "    img = img_to_array(img)\n",
    "    \n",
    "    if mode=='detection':\n",
    "        detect(img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        return\n",
    "        \n",
    "    if mode=='category':\n",
    "        label, confidence = maskPredict(img)\n",
    "        return label\n",
    "        \n",
    "    if mode=='probabilities':\n",
    "        #TODO: sch√∂neres Format\n",
    "        return model.predict(img[None])\n",
    "\n",
    "    else:\n",
    "        print(correct_usage)\n",
    "\n",
    "def detect(img):\n",
    "    faceLocs, rejectLevels, confidences = faceNetLocalize(img)\n",
    "        \n",
    "    for (x, y, w, h) in faceLocs:\n",
    "        #crop image and predict label of cropped image\n",
    "        img_crop = img[y:y+h, x:x+w]\n",
    "        label, confidence_mask = maskPredict(img)\n",
    "        #show label/ bounding box on image\n",
    "        cv2.putText(img, f\"{label}, confidence:{confidence_mask}\", (x+w-30, y+h), cv2.FONT_HERSHEY_PLAIN, 1.0, cv2.CV_RGB(0,255,0), 2.0) \n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow('live_output', img)\n",
    "\n",
    "def live_det():\n",
    "    #TODO: errorhandling for camera\n",
    "    \n",
    "    wait_time = 10 #time in ms to wait before refreshing feed\n",
    "    camera = cv2.VideoCapture(0) #Input value might differ on different systems\n",
    "    \n",
    "    while(True):\n",
    "        _, img = camera.read()\n",
    "\n",
    "        detect(img)\n",
    "\n",
    "        #wait for ESC or q\n",
    "        if (cv2.waitKey(wait_time) & 0xFF) in [27, ord('q')]: \n",
    "            break\n",
    "\n",
    "    camera.release()\n",
    "    return 'live_output'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds, labels, testX, testY, trainX, trainY = load_dataset()\n",
    "#bhps, tuner = tune_model(basic_model_builder, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8/8 [==============================] - 11s 1s/step - loss: 23.4111 - accuracy: 0.3673 - val_loss: 1.9855 - val_accuracy: 0.3934\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.5587 - accuracy: 0.3469 - val_loss: 1.1873 - val_accuracy: 0.2787\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 1.1008 - accuracy: 0.3959 - val_loss: 1.0501 - val_accuracy: 0.4426\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 1.0261 - accuracy: 0.3592 - val_loss: 0.9553 - val_accuracy: 0.4754\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.9040 - accuracy: 0.5633 - val_loss: 0.8631 - val_accuracy: 0.5246\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 1.0122 - accuracy: 0.5551 - val_loss: 0.8599 - val_accuracy: 0.4754\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.8276 - accuracy: 0.6204 - val_loss: 0.7669 - val_accuracy: 0.7705\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.7429 - accuracy: 0.7061 - val_loss: 0.7597 - val_accuracy: 0.6885\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.5802 - accuracy: 0.7796 - val_loss: 0.4395 - val_accuracy: 0.8689\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.5521 - accuracy: 0.7551 - val_loss: 0.3343 - val_accuracy: 0.8852\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.4904 - accuracy: 0.8204 - val_loss: 0.4534 - val_accuracy: 0.8525\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.4720 - accuracy: 0.8082 - val_loss: 0.2458 - val_accuracy: 0.9016\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.3743 - accuracy: 0.8490 - val_loss: 0.4727 - val_accuracy: 0.8361\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.4215 - accuracy: 0.8041 - val_loss: 0.2730 - val_accuracy: 0.9016\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 12s 1s/step - loss: 0.3877 - accuracy: 0.8571 - val_loss: 0.2132 - val_accuracy: 0.9016\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.3395 - accuracy: 0.8694 - val_loss: 0.2694 - val_accuracy: 0.8689\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.3407 - accuracy: 0.8776 - val_loss: 0.2953 - val_accuracy: 0.8525\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.3274 - accuracy: 0.8735 - val_loss: 0.3649 - val_accuracy: 0.8525\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.3124 - accuracy: 0.8898 - val_loss: 1.4041 - val_accuracy: 0.6557\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.5909 - accuracy: 0.8163 - val_loss: 0.4037 - val_accuracy: 0.7869\n"
     ]
    }
   ],
   "source": [
    "model = select_model('vgg_small_model')\n",
    "#model = tuner.hypermodel.build(bhps)\n",
    "history = train_model(model, train_ds, test_ds, epochs=20)\n",
    "\n",
    "#evaluate_model(testX, testY, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39344263076782227, 0.2786885201931, 0.44262295961380005, 0.4754098355770111, 0.5245901346206665, 0.4754098355770111, 0.7704917788505554, 0.688524603843689, 0.868852436542511, 0.8852459192276001, 0.8524590134620667, 0.9016393423080444, 0.8360655903816223, 0.9016393423080444, 0.9016393423080444, 0.868852436542511, 0.8524590134620667, 0.8524590134620667, 0.6557376980781555, 0.7868852615356445]\n"
     ]
    }
   ],
   "source": [
    "print(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_path='../img/ffp2/IMG_1596_(657, 421, 1426, 1426).png'\n",
    "#img_path='../img/no_mask/IMG_1323.JPG_(1025, 427, 984, 984).png'\n",
    "#img_path='../img/op_mask/IMG_1337.JPG_(878, 710, 954, 954).png'\n",
    "#img_path='../img/other_mask/IMG_1327.JPG_(863, 476, 1097, 1097).png'\n",
    "\n",
    "#predict(img_path, 'category', model=model, labels=labels)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "320b063615c881cf99d5bda0a05a87a093cceae5b3aed84278fa7ef3d4b95f2a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
