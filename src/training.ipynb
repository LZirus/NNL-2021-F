{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing opencv haarcascade face detection network\n",
    "https://github.com/opencv/opencv/tree/master/data/haarcascades\n",
    "explained in: https://www.youtube.com/watch?v=7IFhsbfby9s&t=300s (or gitHub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertImg(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def faceNetLocalize(img, **kwargs):\n",
    "    scaleFactor = kwargs.get('scaleFactor', 1.1) #between 1.05 (quality) and 1.4 (speed) recommended (scale of the faces we search for)\n",
    "    minNeighbors = kwargs.get('minNeighbors', 4) #between 3 (quantity) and 6 (quality) recommended\n",
    "    minSize = kwargs.get('minSize', (10, 10)) #min size of a face in the picture\n",
    "    faceNet = kwargs.get('faceNet', init_faceNet())\n",
    "    \n",
    "    img_cvt = convertImg(img)\n",
    "    return faceNet.detectMultiScale3(img_cvt, scaleFactor=scaleFactor, minNeighbors=minNeighbors, minSize=minSize, outputRejectLevels = True)\n",
    "\n",
    "def init_faceNet(**kwargs):\n",
    "    path = kwargs.get('path', 'haarcascade_frontalface_default.xml')\n",
    "    return cv2.CascadeClassifier(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: testing with different model types\n",
    "#for example: eye model + larger bounding box towards the bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask classifier\n",
    "\n",
    "foundation: https://www.tensorflow.org/tutorials/load_data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import MaxPool2D, Conv2D, Input, Dense, Flatten, AveragePooling2D, Dropout\n",
    "import tensorflow.keras.layers as lays\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling, RandomContrast, RandomFlip, RandomRotation\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras import Sequential, losses as lfs\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation\n",
    "https://www.tensorflow.org/tutorials/images/data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = Sequential([\n",
    "  RandomFlip(\"horizontal\"),\n",
    "  RandomRotation(0.4),\n",
    "  RandomContrast(0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_size = (180, 180)\n",
    "img_size_vgg = (224, 224)\n",
    "epochs = 11\n",
    "checkpoint_path = \"mask_model/weights.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "imgs_path = os.path.join('..', 'img')\n",
    "num_classes = 3\n",
    "\n",
    "correct_usage =  'correct usage: \\n' + 'predict([path to image], \\'category\\' \\n' + 'predict([path to image], \\'probabilities\\' \\n' + 'predict([path to image], \\'detection\\' \\n' + 'predict(\\'live_detection\\')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loading dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(**kwargs):\n",
    "    imgs_path = kwargs.get('imgs_path', os.path.join('..', 'img'))\n",
    "    img_size = kwargs.get('img_size', (180, 180))\n",
    "    batch_size = kwargs.get('batch_size', 32)\n",
    "\n",
    "    ##load images/ labels from directory\n",
    "    valid_images = [\".jpg\",\".png\",\".jpeg\",\".JPG\"]\n",
    "    x=[]\n",
    "    y=[]\n",
    "    \n",
    "    for root, dirs, files in os.walk(imgs_path):\n",
    "        for filename in files:\n",
    "            end = os.path.splitext(filename)[1]\n",
    "            if end.lower() not in valid_images:\n",
    "                continue\n",
    "            image = load_img(os.path.join(root, filename), target_size=img_size)\n",
    "            image = img_to_array(image)\n",
    "            \n",
    "            label = os.path.join(root, filename).split(os.path.sep)[-2]\n",
    "            \n",
    "            x.append(image)\n",
    "            y.append(label)\n",
    "    x = np.array(x, dtype=\"float32\")\n",
    "    y = np.array(y)\n",
    "\n",
    "    ## convert labels to ints from 0 ... len(labels)-1\n",
    "    labels = []\n",
    "    for i in range(len(y)):\n",
    "        try:\n",
    "            j = labels.index(y[i])\n",
    "        except:\n",
    "            labels.append(y[i])\n",
    "            j = labels.index(y[i])\n",
    "        y[i] = j\n",
    "    y.astype(int)\n",
    "    \n",
    "    ## split dataset\n",
    "    trainX, testX, trainY, testY = train_test_split(x, y, test_size=0.2, random_state=3)\n",
    "\n",
    "    ## one-hot encoding\n",
    "    trainY = utils.to_categorical(trainY, num_classes)\n",
    "    testY = utils.to_categorical(testY, num_classes)\n",
    "\n",
    "    ## data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization=True,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    ## merge xs and ys\n",
    "    train_batches = datagen.flow(trainX, trainY, batch_size=32, subset='training')\n",
    "    test_batches  = datagen.flow(trainX, trainY, batch_size=32, subset='validation')\n",
    "    \n",
    "    return train_batches, test_batches, labels, testX, testY, trainX, trainY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypermodels\n",
    "#https://www.analyticsvidhya.com/blog/2021/06/create-convolutional-neural-network-model-and-optimize-using-keras-tuner-deep-learning/\n",
    "#https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "\n",
    "def basic_model_builder(hp):\n",
    "  \n",
    "    basic_model = Sequential([\n",
    "        augmentation,\n",
    "        Conv2D(filters=hp.Int('c1_filter', min_value=32, max_value=256, step=16), kernel_size=hp.Choice('c1_kernel', values=[3,5]), activation='relu'),\n",
    "        AveragePooling2D(pool_size=(7,7)),\n",
    "        Flatten(name=\"flatten\"),\n",
    "        Dense(units=hp.Int('d1_units', min_value=32, max_value=512, step=32), activation=\"relu\"),\n",
    "        Dropout(0.2),#drops small confidences\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    basic_model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), loss=lfs.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    return basic_model\n",
    "\n",
    "def small_model_builder(hp):\n",
    "  \n",
    "    small_model = Sequential([\n",
    "        augmentation,\n",
    "        Conv2D(filters=hp.Int('c1_filter', min_value=32, max_value=256, step=16), kernel_size=hp.Choice('c1_kernel', values=[3,5]), activation='relu'),\n",
    "        Conv2D(filters=hp.Int('c2_filter', min_value=32, max_value=256, step=16), kernel_size=hp.Choice('c2_kernel', values=[3,5]), activation='relu'),\n",
    "        MaxPool2D(pool_size=(3,3)),\n",
    "        Flatten(name=\"flatten\"),\n",
    "        Dense(units=hp.Int('d1_units', min_value=32, max_value=512, step=32), activation=\"relu\"),\n",
    "        Dropout(0.2),#drops small confidences\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    small_model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), loss=lfs.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    return small_model\n",
    "\n",
    "\n",
    "def tune_model(model_builder, xs, ys):\n",
    "    #tuner = kt.RandomSearch(model_builder, objective='val_accuracy', max_trials=5)\n",
    "    tuner = kt.RandomSearch(kt.applications.HyperResNet(input_shape=(180, 180, 3), classes=2), objective='val_loss', max_trials=5)\n",
    "    tuner = kt.Hyperband(model_builder, objective='val_accuracy', max_epochs=10, factor=3)\n",
    "    tuner.search(xs, ys, epochs=50, validation_split=0.2)\n",
    "    return tuner.get_best_hyperparameters(num_trials=1)[0], tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(model_name, **kwargs):\n",
    "    num_labels = kwargs.get('num_classes', num_classes)\n",
    "\n",
    "    basic_model = Sequential()\n",
    "    basic_model.add(Rescaling(1. /255))\n",
    "    basic_model.add(Conv2D(32, (3,3), activation='relu', input_shape=(180,180,3)))\n",
    "    basic_model.add(AveragePooling2D(pool_size=(7,7)))\n",
    "    basic_model.add(Flatten(name=\"flatten\"))\n",
    "    basic_model.add(Dense(128, activation=\"relu\"))\n",
    "    basic_model.add(Dropout(0.5)) #drops small confidences\n",
    "    basic_model.add(Dense(num_labels, activation=\"softmax\"))\n",
    "    \n",
    "\n",
    "    small_model = Sequential()\n",
    "    small_model.add(Rescaling(1. /255))\n",
    "    small_model.add(Conv2D(filters=128, kernel_size=(5,5), activation='relu', input_shape=(180,180,3)))\n",
    "    small_model.add(Conv2D(filters=128, kernel_size=(5,5), activation='relu'))\n",
    "    small_model.add(MaxPool2D(pool_size=(3,3)))\n",
    "    small_model.add(Flatten(name=\"flatten\"))\n",
    "    small_model.add(Dense(units=224, activation=\"relu\"))\n",
    "    small_model.add(Dropout(0.5))#drops small confidences\n",
    "    small_model.add(Dense(num_labels, activation=\"softmax\"))\n",
    "    \n",
    "\n",
    "    vgg_small_model=Sequential()\n",
    "    \n",
    "    vgg_small_model.add(Conv2D(64,(3,3),activation='relu',input_shape=(180,180,3)))\n",
    "    vgg_small_model.add(MaxPool2D(2,2))\n",
    "    vgg_small_model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "    vgg_small_model.add(MaxPool2D(2,2))\n",
    "    vgg_small_model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "    vgg_small_model.add(MaxPool2D(2,2))\n",
    "    vgg_small_model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "    vgg_small_model.add(MaxPool2D(2,2))\n",
    "    vgg_small_model.add(Flatten())\n",
    "    vgg_small_model.add(Dropout(0.5))\n",
    "    vgg_small_model.add(Dense(120,activation='relu'))\n",
    "    vgg_small_model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "    vgg_smaller_model=Sequential()\n",
    "    \n",
    "    vgg_smaller_model.add(Conv2D(64,(3,3),activation='relu',input_shape=(180,180,3)))\n",
    "    vgg_smaller_model.add(MaxPool2D(2,2))\n",
    "    vgg_smaller_model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "    vgg_smaller_model.add(MaxPool2D(2,2))\n",
    "    vgg_smaller_model.add(Flatten())\n",
    "    vgg_smaller_model.add(Dropout(0.5))\n",
    "    vgg_smaller_model.add(Dense(120,activation='relu'))\n",
    "    vgg_smaller_model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "    vgg_model = Sequential()\n",
    "    vgg_model.add(Rescaling(1. /255))\n",
    "    vgg_model.add(Conv2D(input_shape=(224,224,3), filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", strides=(1,1))) \n",
    "    vgg_model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    vgg_model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(MaxPool2D(pool_size=(2, 2), strides=(2)))\n",
    "    vgg_model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(MaxPool2D(pool_size=(2, 2), strides=(2)))\n",
    "    vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(MaxPool2D(pool_size=(2, 2), strides=(2)))\n",
    "    vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(MaxPool2D(pool_size=(2, 2), strides=(2)))\n",
    "    vgg_model.add(Flatten())\n",
    "    vgg_model.add(Dense(units=4096, activation=\"relu\"))\n",
    "    vgg_model.add(Dense(units=4096, activation=\"relu\"))\n",
    "    vgg_model.add(Dense(units=num_labels, activation=\"softmax\"))\n",
    "    \n",
    "\n",
    "    if model_name == 'basic_model':\n",
    "        #basic_model.summary()\n",
    "        return basic_model\n",
    "    \n",
    "    if model_name == 'small_model':\n",
    "        #small_model.summary()\n",
    "        return small_model\n",
    "\n",
    "    if model_name == 'vgg_model':\n",
    "        #vgg_model.summary()\n",
    "        return vgg_model\n",
    "    \n",
    "    if model_name == 'vgg_small_model':\n",
    "        #vgg_small_model.summary()\n",
    "        return vgg_small_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about the VGG-model: https://neurohive.io/en/popular-networks/vgg16/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_ds, val_ds, **kwargs):\n",
    "    epochs = kwargs.get('epochs', 10)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation (mask):\n",
    "https://www.tensorflow.org/guide/keras/train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(x_test, y_test, model):\n",
    "    results = model.evaluate(x_test, y_test, batch_size=32)\n",
    "    print(results)\n",
    "    \n",
    "    y_pred_confidences = model.predict(x_test)\n",
    "    y_pred = [np.argmax(cs) for cs in y_pred_confidences]\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(**kwargs):\n",
    "    checkpoint_path = kwargs.get('checkpoint_path', \"mask_model/weights.ckpt\")\n",
    "    model = kwargs.get('model', select_model('basic_model'))\n",
    "    model.load_weights(checkpoint_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskPredict(model, img, labels):\n",
    "    pred = model.predict(img[None])\n",
    "    label_index = np.argmax(pred)\n",
    "    print(labels[label_index])\n",
    "    return labels[label_index], pred[0][label_index]\n",
    "\n",
    "\n",
    "#mode can be 'category', 'probabilities', 'detection', 'live_detection'\n",
    "def predict(mode, model, **kwargs):\n",
    "    #model = kwargs.get('model', load_model())\n",
    "    img_path = kwargs.get('img_path', None)\n",
    "    img_size = kwargs.get('img_size', (180, 180))\n",
    "    labels = kwargs.get('labels', None)\n",
    "    \n",
    "    \n",
    "    if mode=='live_detection':\n",
    "        live_det(model, img_size, labels)\n",
    "        return\n",
    "    \n",
    "    if img_path is None:\n",
    "        print(correct_usage)\n",
    "        return\n",
    "\n",
    "    #load_img\n",
    "    img = load_img(img_path, target_size = img_size)\n",
    "    img = img_to_array(img)\n",
    "    \n",
    "    if mode=='detection':\n",
    "        detect(img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        return\n",
    "        \n",
    "    if mode=='category':\n",
    "        label, confidence = maskPredict(model, img, labels)\n",
    "        return label\n",
    "        \n",
    "    if mode=='probabilities':\n",
    "        #TODO: schöneres Format\n",
    "        return model.predict(img[None])\n",
    "\n",
    "    else:\n",
    "        print(correct_usage)\n",
    "\n",
    "def detect(model, img, img_size, labels):\n",
    "    faceLocs, rejectLevels, confidences = faceNetLocalize(img)\n",
    "        \n",
    "    for (x, y, w, h) in faceLocs:\n",
    "        #crop image and predict label of cropped image\n",
    "        img_crop = img[y:y+w, x:x+h]\n",
    "        img_crop = cv2.resize(img_crop, img_size)\n",
    "        img_crop = img_crop / 255\n",
    "        label, confidence_mask = maskPredict(model, img_to_array(img_crop), labels)\n",
    "\n",
    "        #show label/ bounding box on image\n",
    "        cv2.putText(img, f\"{label}, confidence:{confidence_mask}\", (x, y), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,0), 2) \n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 3)\n",
    "    \n",
    "    cv2.imshow('live_output', img)\n",
    "\n",
    "def live_det(model, img_size, labels):\n",
    "    wait_time = 10 #time in ms to wait before refreshing feed\n",
    "    camera = cv2.VideoCapture(0) #Input value might differ on different systems\n",
    "    \n",
    "    while(True):\n",
    "        ret, img = camera.read()\n",
    "        if not ret:\n",
    "            print('Error: failed reading camera')\n",
    "            return 'Error: failed reading camera'\n",
    "        detect(model, img, img_size, labels)\n",
    "\n",
    "        #wait for ESC or q\n",
    "        if (cv2.waitKey(wait_time) & 0xFF) in [27, ord('q')]: \n",
    "            break\n",
    "\n",
    "    camera.release()\n",
    "    return 'live_output'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds, labels, testX, testY, trainX, trainY = load_dataset()\n",
    "#bhps, tuner = tune_model(basic_model_builder, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8/8 [==============================] - 5s 586ms/step - loss: 2.3970 - accuracy: 0.2571 - val_loss: 0.9822 - val_accuracy: 0.4754\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 4s 548ms/step - loss: 1.0546 - accuracy: 0.4367 - val_loss: 0.9217 - val_accuracy: 0.6721\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 4s 523ms/step - loss: 0.9066 - accuracy: 0.6286 - val_loss: 0.8746 - val_accuracy: 0.6393\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 4s 512ms/step - loss: 0.8395 - accuracy: 0.6490 - val_loss: 0.7718 - val_accuracy: 0.7213\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 4s 515ms/step - loss: 0.7730 - accuracy: 0.6408 - val_loss: 0.7504 - val_accuracy: 0.6230\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 4s 512ms/step - loss: 0.7301 - accuracy: 0.7265 - val_loss: 0.6239 - val_accuracy: 0.8361\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 4s 509ms/step - loss: 0.6949 - accuracy: 0.7102 - val_loss: 0.5758 - val_accuracy: 0.8197\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 4s 500ms/step - loss: 0.6646 - accuracy: 0.7020 - val_loss: 0.5477 - val_accuracy: 0.8525\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 4s 512ms/step - loss: 0.6187 - accuracy: 0.7306 - val_loss: 0.5253 - val_accuracy: 0.8689\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 4s 503ms/step - loss: 0.5960 - accuracy: 0.7347 - val_loss: 0.5514 - val_accuracy: 0.8033\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 4s 494ms/step - loss: 0.6011 - accuracy: 0.7388 - val_loss: 0.5236 - val_accuracy: 0.8525\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 4s 515ms/step - loss: 0.6316 - accuracy: 0.7429 - val_loss: 0.4637 - val_accuracy: 0.8852\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 4s 508ms/step - loss: 0.5675 - accuracy: 0.7592 - val_loss: 0.4903 - val_accuracy: 0.8689\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 4s 516ms/step - loss: 0.4901 - accuracy: 0.8041 - val_loss: 0.4439 - val_accuracy: 0.8197\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 4s 516ms/step - loss: 0.5262 - accuracy: 0.7878 - val_loss: 0.4101 - val_accuracy: 0.8689\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 4s 528ms/step - loss: 0.5558 - accuracy: 0.7265 - val_loss: 0.4112 - val_accuracy: 0.8852\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 4s 497ms/step - loss: 0.4815 - accuracy: 0.7837 - val_loss: 0.4665 - val_accuracy: 0.8033\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 4s 509ms/step - loss: 0.5260 - accuracy: 0.7551 - val_loss: 0.3507 - val_accuracy: 0.9344\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 4s 520ms/step - loss: 0.5022 - accuracy: 0.7755 - val_loss: 0.4116 - val_accuracy: 0.8525\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 4s 512ms/step - loss: 0.4851 - accuracy: 0.8367 - val_loss: 0.5271 - val_accuracy: 0.8361\n"
     ]
    }
   ],
   "source": [
    "model = select_model('basic_model')\n",
    "#model = tuner.hypermodel.build(bhps)\n",
    "history = train_model(model, train_ds, test_ds, epochs=20)\n",
    "\n",
    "#evaluate_model(testX, testY, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = [2.3970251083374023, 1.0545551776885986, 0.9066411852836609, 0.8395401239395142, 0.7729923725128174, 0.730128824710846, 0.6948890686035156, 0.6645703315734863, 0.6187208294868469, 0.5959728360176086, 0.60105961561203, 0.6316047310829163, 0.5675155520439148, 0.49010926485061646, 0.5262033939361572, 0.5557922124862671, 0.4815269708633423, 0.5259525179862976, 0.5021576285362244, 0.4851398468017578]\n",
      "val_loss = [0.9821628332138062, 0.921679675579071, 0.8746117949485779, 0.7717764377593994, 0.7503803372383118, 0.6238858699798584, 0.5757790207862854, 0.5477417707443237, 0.5252807140350342, 0.5514279007911682, 0.5235510468482971, 0.4636566936969757, 0.4903154671192169, 0.4438585638999939, 0.4100685119628906, 0.41123974323272705, 0.4665105640888214, 0.35066360235214233, 0.4116128087043762, 0.5270665884017944]\n",
      "acc = [0.2571428716182709, 0.43673470616340637, 0.6285714507102966, 0.6489796042442322, 0.640816330909729, 0.7265306115150452, 0.7102040648460388, 0.7020407915115356, 0.7306122183799744, 0.7346938848495483, 0.7387754917144775, 0.7428571581840515, 0.7591836452484131, 0.8040816187858582, 0.7877551317214966, 0.7265306115150452, 0.7836734652519226, 0.7551020383834839, 0.7755101919174194, 0.8367347121238708]\n",
      "val_acc = [0.4754098355770111, 0.6721311211585999, 0.6393442749977112, 0.7213114500045776, 0.6229507923126221, 0.8360655903816223, 0.8196721076965332, 0.8524590134620667, 0.868852436542511, 0.8032786846160889, 0.8524590134620667, 0.8852459192276001, 0.868852436542511, 0.8196721076965332, 0.868852436542511, 0.8852459192276001, 0.8032786846160889, 0.9344262480735779, 0.8524590134620667, 0.8360655903816223]\n"
     ]
    }
   ],
   "source": [
    "print(\"loss =\",history.history[\"loss\"])\n",
    "print(\"val_loss =\",history.history[\"val_loss\"])\n",
    "print(\"acc =\",history.history[\"accuracy\"])\n",
    "print(\"val_acc =\",history.history[\"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n"
     ]
    }
   ],
   "source": [
    "img_path='../img/ffp2/IMG_1599.JPG_(350.0, 92.0, 1009.0, 599.0).png'\n",
    "#img_path='../img/no_mask/IMG_1323.JPG_(1025, 427, 984, 984).png'\n",
    "#img_path='../img/op_mask/IMG_1337.JPG_(878, 710, 954, 954).png'\n",
    "#img_path='../img/other_mask/IMG_1327.JPG_(863, 476, 1097, 1097).png'\n",
    "predict('category',img_path=img_path,  model=model, labels=labels, img_size=(180,180))\n",
    "\n",
    "#predict('live_detection', model=model, labels=labels, img_size=(180,180))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "320b063615c881cf99d5bda0a05a87a093cceae5b3aed84278fa7ef3d4b95f2a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
