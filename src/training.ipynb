{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing opencv haarcascade face detection network\n",
    "https://github.com/opencv/opencv/tree/master/data/haarcascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports necessary for the face detection network\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts a loaded image into the correct format for the openCV face detection network\n",
    "def convertImg(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# uses the FaceNet to localize faces in a picture\n",
    "def faceNetLocalize(img, **kwargs):\n",
    "    #parameters\n",
    "    scaleFactor = kwargs.get('scaleFactor', 1.1)    # between 1.05 (quality) and 1.4 (speed) recommended (scale of the faces we search for)\n",
    "    minNeighbors = kwargs.get('minNeighbors', 4)    # between 3 (quantity) and 6 (quality) recommended\n",
    "    minSize = kwargs.get('minSize', (10, 10))       # min size of a face in the picture\n",
    "    faceNet = kwargs.get('faceNet', init_faceNet()) # pretrained model from openCV\n",
    "    \n",
    "    # conversion and localization\n",
    "    img_cvt = convertImg(img)\n",
    "    return faceNet.detectMultiScale3(img_cvt, scaleFactor=scaleFactor, minNeighbors=minNeighbors, minSize=minSize, outputRejectLevels = True)\n",
    "\n",
    "# initialize the network using openCV\n",
    "def init_faceNet(**kwargs):\n",
    "    path = kwargs.get('path', 'haarcascade_frontalface_default.xml')\n",
    "    return cv2.CascadeClassifier(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: testing with different model types\n",
    "#for example: eye model + larger bounding box towards the bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask classifier\n",
    "\n",
    "foundation: https://www.tensorflow.org/tutorials/load_data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports necessary for the mask classifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import MaxPool2D, Conv2D, Dense, Flatten, AveragePooling2D, Dropout\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras import Sequential, losses as lfs\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import utils, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful variables\n",
    "\n",
    "batch_size = 32                             # the amount of images processed at once\n",
    "img_size = (180, 180)                       # img_size in pixels\n",
    "img_size_vgg = (224, 224)                   # the vgg network needs a fixed img_size\n",
    "def_epochs = 11                                 # default epochs used in training - should be chosen wisely\n",
    "imgs_path = os.path.join('..', 'img')       # path to the dataset\n",
    "num_classes = 3                             # default number of classes/ labels\n",
    "\n",
    "# String for the correct usage of predict()\n",
    "correct_usage = ('correct usage: \\n' \n",
    "                'predict(\\'category\\', [model], [labels], img_path=[img_path]) \\n'\n",
    "                'predict(\\'probabilities\\', [model], [labels], img_path=[img_path]) \\n'\n",
    "                'predict(\\'detection\\', [model], [labels], img_path=[img_path])\\n'\n",
    "                'predict(\\'live_detection\\', [model], [labels])'\n",
    "                'optional parameter: img_size, default=(180,180)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loading dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to load the images\n",
    "def load_dataset(**kwargs):\n",
    "    # optional values\n",
    "    imgs_path = kwargs.get('imgs_path', os.path.join('..', 'img'))\n",
    "    img_size = kwargs.get('img_size', (180, 180))\n",
    "\n",
    "    # initializing lists\n",
    "    valid_images = [\".jpg\",\".png\",\".jpeg\",\".JPG\"]\n",
    "    x=[]\n",
    "    y=[]\n",
    "    \n",
    "    # loop over files in image directory\n",
    "    for root, dirs, files in os.walk(imgs_path):\n",
    "        for filename in files:\n",
    "            end = os.path.splitext(filename)[1]\n",
    "            if end.lower() not in valid_images:\n",
    "                continue\n",
    "            image = load_img(os.path.join(root, filename), target_size=img_size)\n",
    "            image = img_to_array(image)\n",
    "            \n",
    "            label = os.path.join(root, filename).split(os.path.sep)[-2]\n",
    "            \n",
    "            x.append(image)\n",
    "            y.append(label)\n",
    "    \n",
    "    # convert images and labels to arrays for further use\n",
    "    x = np.array(x, dtype=\"float32\")\n",
    "    y = np.array(y)\n",
    "\n",
    "    ## convert labels to ints from 0 ... len(labels)-1\n",
    "    labels = []\n",
    "    for i in range(len(y)):\n",
    "        try:\n",
    "            j = labels.index(y[i])\n",
    "        except:\n",
    "            labels.append(y[i])\n",
    "            j = labels.index(y[i])\n",
    "        y[i] = j\n",
    "    y.astype(int)\n",
    "    \n",
    "    ## split dataset\n",
    "    trainX, testX, trainY, testY = train_test_split(x, y, test_size=0.2, random_state=3)\n",
    "\n",
    "    ## one-hot encoding\n",
    "    trainY = utils.to_categorical(trainY, num_classes)\n",
    "    testY = utils.to_categorical(testY, num_classes)\n",
    "\n",
    "    ## data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization=True,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    ## merge xs and ys\n",
    "    train_batches = datagen.flow(trainX, trainY, batch_size=32, subset='training')\n",
    "    test_batches  = datagen.flow(trainX, trainY, batch_size=32, subset='validation')\n",
    "    \n",
    "    return train_batches, test_batches, labels, testX, testY, trainX, trainY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypermodels\n",
    "# we tried using them, but in the end, they were not useful enough\n",
    "\n",
    "#https://www.analyticsvidhya.com/blog/2021/06/create-convolutional-neural-network-model-and-optimize-using-keras-tuner-deep-learning/\n",
    "#https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "\n",
    "# hypermodel using the same layers as the basic_model\n",
    "def basic_model_builder(hp):\n",
    "  \n",
    "    basic_model = Sequential([\n",
    "        Conv2D(filters=hp.Int('c1_filter', min_value=32, max_value=256, step=16), kernel_size=hp.Choice('c1_kernel', values=[3,5]), activation='relu'),\n",
    "        AveragePooling2D(pool_size=(7,7)),\n",
    "        Flatten(name=\"flatten\"),\n",
    "        Dense(units=hp.Int('d1_units', min_value=32, max_value=512, step=32), activation=\"relu\"),\n",
    "        Dropout(0.2),#drops small confidences\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    basic_model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), loss=lfs.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    return basic_model\n",
    "\n",
    "# hypermodel using the same layers as the small_model\n",
    "def small_model_builder(hp):\n",
    "  \n",
    "    small_model = Sequential([\n",
    "        Conv2D(filters=hp.Int('c1_filter', min_value=32, max_value=256, step=16), kernel_size=hp.Choice('c1_kernel', values=[3,5]), activation='relu'),\n",
    "        Conv2D(filters=hp.Int('c2_filter', min_value=32, max_value=256, step=16), kernel_size=hp.Choice('c2_kernel', values=[3,5]), activation='relu'),\n",
    "        MaxPool2D(pool_size=(3,3)),\n",
    "        Flatten(name=\"flatten\"),\n",
    "        Dense(units=hp.Int('d1_units', min_value=32, max_value=512, step=32), activation=\"relu\"),\n",
    "        Dropout(0.2),#drops small confidences\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    small_model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), loss=lfs.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    return small_model\n",
    "\n",
    "# utilization of the keras auto-tuner\n",
    "def tune_model(model_builder, xs, ys):\n",
    "    tuner = kt.RandomSearch(kt.applications.HyperResNet(input_shape=(180, 180, 3), classes=2), objective='val_loss', max_trials=5)\n",
    "    tuner = kt.Hyperband(model_builder, objective='val_accuracy', max_epochs=10, factor=3)\n",
    "    tuner.search(xs, ys, epochs=50, validation_split=0.2)\n",
    "    return tuner.get_best_hyperparameters(num_trials=1)[0], tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to select a model structure by name\n",
    "def select_model(model_name, **kwargs):\n",
    "    #optional parameters\n",
    "    num_labels = kwargs.get('num_classes', num_classes) # number of labels: is necessary for a correct output on the last Dense layer\n",
    "\n",
    "    # simplest model\n",
    "    basic_model = Sequential()\n",
    "    basic_model.add(Conv2D(32, (3,3), activation='relu', input_shape=(180,180,3)))\n",
    "    basic_model.add(AveragePooling2D(pool_size=(7,7)))\n",
    "    basic_model.add(Flatten(name=\"flatten\"))\n",
    "    basic_model.add(Dense(128, activation=\"relu\"))\n",
    "    basic_model.add(Dropout(0.5)) #drops small confidences\n",
    "    basic_model.add(Dense(num_labels, activation=\"softmax\"))\n",
    "    \n",
    "    #more complicated model, utilizing 2 convolutional layers like the vgg - still very time-consuming in training\n",
    "    small_model = Sequential()\n",
    "    small_model.add(Conv2D(filters=128, kernel_size=(5,5), activation='relu', input_shape=(180,180,3)))\n",
    "    small_model.add(Conv2D(filters=128, kernel_size=(5,5), activation='relu'))\n",
    "    small_model.add(MaxPool2D(pool_size=(3,3)))\n",
    "    small_model.add(Flatten(name=\"flatten\"))\n",
    "    small_model.add(Dense(units=224, activation=\"relu\"))\n",
    "    small_model.add(Dropout(0.5))#drops small confidences\n",
    "    small_model.add(Dense(num_labels, activation=\"softmax\"))\n",
    "    \n",
    "    #smaller version of the \"vgg_small_model\"\n",
    "    vgg_smaller_model=Sequential()\n",
    "    vgg_smaller_model.add(Conv2D(64,(3,3),activation='relu',input_shape=(180,180,3)))\n",
    "    vgg_smaller_model.add(MaxPool2D(2,2))\n",
    "    vgg_smaller_model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "    vgg_smaller_model.add(MaxPool2D(2,2))\n",
    "    vgg_smaller_model.add(Flatten())\n",
    "    vgg_smaller_model.add(Dropout(0.5))\n",
    "    vgg_smaller_model.add(Dense(120,activation='relu'))\n",
    "    vgg_smaller_model.add(Dense(num_labels,activation='softmax'))\n",
    "    \n",
    "    # simpler version of the vgg model, utilizes only one convolutional layer at a time, before max-pooling\n",
    "    # but keeps the general design of the vgg model\n",
    "    vgg_small_model=Sequential()\n",
    "    vgg_small_model.add(Conv2D(64,(3,3),activation='relu',input_shape=(img_size[0],img_size[1],3)))\n",
    "    vgg_small_model.add(MaxPool2D(2,2))\n",
    "    vgg_small_model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "    vgg_small_model.add(MaxPool2D(2,2))\n",
    "    vgg_small_model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "    vgg_small_model.add(MaxPool2D(2,2))\n",
    "    vgg_small_model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "    vgg_small_model.add(MaxPool2D(2,2))\n",
    "    vgg_small_model.add(Flatten())\n",
    "    vgg_small_model.add(Dropout(0.5))\n",
    "    vgg_small_model.add(Dense(120,activation='relu'))\n",
    "    vgg_small_model.add(Dense(num_labels,activation='softmax'))\n",
    "\n",
    "\n",
    "    # the vgg model is a state of the art model design, we do not have the necessary power or training data to utilize this model\n",
    "    vgg_model = Sequential()\n",
    "    vgg_model.add(Conv2D(input_shape=(224,224,3), filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", strides=(1,1))) \n",
    "    vgg_model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    vgg_model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(MaxPool2D(pool_size=(2, 2), strides=(2)))\n",
    "    vgg_model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(MaxPool2D(pool_size=(2, 2), strides=(2)))\n",
    "    vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(MaxPool2D(pool_size=(2, 2), strides=(2)))\n",
    "    vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    vgg_model.add(MaxPool2D(pool_size=(2, 2), strides=(2)))\n",
    "    vgg_model.add(Flatten())\n",
    "    vgg_model.add(Dense(units=4096, activation=\"relu\"))\n",
    "    vgg_model.add(Dense(units=4096, activation=\"relu\"))\n",
    "    vgg_model.add(Dense(units=num_labels, activation=\"softmax\"))\n",
    "    \n",
    "\n",
    "    # returns the correct model\n",
    "\n",
    "    if model_name == 'basic_model':\n",
    "        #basic_model.summary()\n",
    "        return basic_model\n",
    "    \n",
    "    if model_name == 'small_model':\n",
    "        #small_model.summary()\n",
    "        return small_model\n",
    "\n",
    "    if model_name == 'vgg_model':\n",
    "        #vgg_model.summary()\n",
    "        return vgg_model\n",
    "    \n",
    "    if model_name == 'vgg_small_model':\n",
    "        #vgg_small_model.summary()\n",
    "        return vgg_small_model\n",
    "    \n",
    "    if model_name == 'vgg_smaller_model':\n",
    "        #vgg_smaller_model.summary()\n",
    "        return vgg_smaller_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about the VGG-model: https://neurohive.io/en/popular-networks/vgg16/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model using the given dataset\n",
    "def train_model(model, train_ds, val_ds, **kwargs):\n",
    "    # optional parameter\n",
    "    epochs = kwargs.get('epochs', def_epochs)   # training epochs\n",
    "    \n",
    "    # some loss functions we tested:\n",
    "    # categorical_crossentropy\n",
    "    # mean_squared_error\n",
    "    # mean_squared_logarithmic_error\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # train/fit the model and save the history for documentation/ testing purposes\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation (mask):\n",
    "https://www.tensorflow.org/guide/keras/train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deprecated evaluation function, but could still be useful\n",
    "def evaluate_model(x_test, y_test, model):\n",
    "    y_pred_confidences = model.predict(x_test)\n",
    "    y_pred = [np.argmax(cs) for cs in y_pred_confidences]\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load/ Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(path, model):\n",
    "    model.save(path, save_format=\"h5\")\n",
    "    \n",
    "def load_model_good(path):\n",
    "    return models.load_model(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the label with the highest confidence of a prediction\n",
    "def maskPredict(model, img, labels):\n",
    "    pred = model.predict(img[None])\n",
    "    label_index = np.argmax(pred)\n",
    "    print(labels[label_index])\n",
    "    return labels[label_index], pred[0][label_index]\n",
    "\n",
    "\n",
    "#mode can be 'category', 'probabilities', 'detection', 'live_detection'\n",
    "def predict(mode, model, **kwargs):\n",
    "    #model = kwargs.get('model', load_model())\n",
    "    img_path = kwargs.get('img_path', None)\n",
    "    img_size = kwargs.get('img_size', (180, 180))\n",
    "    labels = kwargs.get('labels', None)\n",
    "    \n",
    "    # display the help menu \n",
    "    if mode=='help':\n",
    "        print(correct_usage)\n",
    "        return\n",
    "\n",
    "    # change to live_detection and interpret camera feed\n",
    "    if mode=='live_detection':\n",
    "        live_det(model, img_size, labels)\n",
    "        return\n",
    "    \n",
    "    # print help menu, as function was malused\n",
    "    if img_path is None:\n",
    "        print(correct_usage)\n",
    "        return\n",
    "\n",
    "    #load_img\n",
    "    img = load_img(img_path, target_size = img_size)\n",
    "    img = img_to_array(img)\n",
    "    \n",
    "    #change into detection mode\n",
    "    if mode=='detection':\n",
    "        detect(img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        return\n",
    "        \n",
    "    # change into category mode, printing the label of highest confidence\n",
    "    if mode=='category':\n",
    "        label, _ = maskPredict(model, img, labels)\n",
    "        return label\n",
    "        \n",
    "    # change into probability mode, printing the probability for each label\n",
    "    if mode=='probabilities':\n",
    "        confidences = model.predict(img[None])[0]\n",
    "        ret_str = ''\n",
    "        for lab in range(len(labels)):\n",
    "          ret_str = ret_str + (f\"{labels[lab]}: {confidences[lab]}\")\n",
    "        return ret_str\n",
    "\n",
    "    # no valid mode was specified: print help menu\n",
    "    else:\n",
    "        print(correct_usage)\n",
    "\n",
    "# detect and classify a face on an image\n",
    "def detect(model, img, img_size, labels):\n",
    "    faceLocs, rejectLevels, confidences = faceNetLocalize(img)\n",
    "        \n",
    "    for (x, y, w, h) in faceLocs:\n",
    "        #crop image and predict label of cropped image\n",
    "        img_crop = img[y:y+w, x:x+h]\n",
    "        img_crop = cv2.resize(img_crop, img_size)\n",
    "        img_crop = img_crop / 255\n",
    "        label, confidence_mask = maskPredict(model, img_to_array(img_crop), labels)\n",
    "\n",
    "        #show label/ bounding box on image\n",
    "        cv2.putText(img, f\"{label}, confidence:{confidence_mask}\", (x, y), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,0), 2) \n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 3)\n",
    "    \n",
    "    cv2.imshow('live_output', img)\n",
    "\n",
    "# detect and classify images from a live-feed\n",
    "def live_det(model, img_size, labels):\n",
    "    wait_time = 10 #time in ms to wait before refreshing feed\n",
    "    camera = cv2.VideoCapture(0) #Input value might differ on different systems\n",
    "    \n",
    "    while(True):\n",
    "        ret, img = camera.read()\n",
    "        if not ret:\n",
    "            print('Error: failed reading camera')\n",
    "            return 'Error: failed reading camera'\n",
    "        detect(model, img, img_size, labels)\n",
    "\n",
    "        #wait for ESC or q\n",
    "        if (cv2.waitKey(wait_time) & 0xFF) in [27, ord('q')]: \n",
    "            break\n",
    "\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return 'live_output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing, documentation, stuff ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds, labels, testX, testY, trainX, trainY = load_dataset(img_size=img_size)\n",
    "#bhps, tuner = tune_model(basic_model_builder, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "8/8 [==============================] - 14s 1s/step - loss: 0.2241 - accuracy: 0.3918 - val_loss: 0.2090 - val_accuracy: 0.4754\n",
      "Epoch 2/11\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.2178 - accuracy: 0.3837 - val_loss: 0.2088 - val_accuracy: 0.6721\n",
      "Epoch 3/11\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.1911 - accuracy: 0.6204 - val_loss: 0.1466 - val_accuracy: 0.7049\n",
      "Epoch 4/11\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.1495 - accuracy: 0.6449 - val_loss: 0.1237 - val_accuracy: 0.8033\n",
      "Epoch 5/11\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.1298 - accuracy: 0.6735 - val_loss: 0.1251 - val_accuracy: 0.7049\n",
      "Epoch 6/11\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.1389 - accuracy: 0.6857 - val_loss: 0.1150 - val_accuracy: 0.7869\n",
      "Epoch 7/11\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.1210 - accuracy: 0.7429 - val_loss: 0.1026 - val_accuracy: 0.7541\n",
      "Epoch 8/11\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.1094 - accuracy: 0.7714 - val_loss: 0.1172 - val_accuracy: 0.7213\n",
      "Epoch 9/11\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.0895 - accuracy: 0.8041 - val_loss: 0.0715 - val_accuracy: 0.8852\n",
      "Epoch 10/11\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.0786 - accuracy: 0.8571 - val_loss: 0.0798 - val_accuracy: 0.8361\n",
      "Epoch 11/11\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.0750 - accuracy: 0.8408 - val_loss: 0.0590 - val_accuracy: 0.8852\n"
     ]
    }
   ],
   "source": [
    "model = select_model('vgg_small_model')\n",
    "#model = tuner.hypermodel.build(bhps)\n",
    "history = train_model(model, train_ds, test_ds, epochs=11)\n",
    "\n",
    "#evaluate_model(testX, testY, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = [0.22412164509296417, 0.2177887111902237, 0.19107238948345184, 0.14946597814559937, 0.1298074722290039, 0.13892722129821777, 0.12100914120674133, 0.10939215868711472, 0.08949562907218933, 0.0785859078168869, 0.07500513643026352]\n",
      "val_loss = [0.20902186632156372, 0.20877425372600555, 0.14655490219593048, 0.12370990961790085, 0.12510359287261963, 0.11497010290622711, 0.10255080461502075, 0.11720287799835205, 0.07151245325803757, 0.07978873699903488, 0.05898880586028099]\n",
      "acc = [0.3918367326259613, 0.38367345929145813, 0.6204081773757935, 0.6448979377746582, 0.6734693646430969, 0.6857143044471741, 0.7428571581840515, 0.7714285850524902, 0.8040816187858582, 0.8571428656578064, 0.8408163189888]\n",
      "val_acc = [0.4754098355770111, 0.6721311211585999, 0.7049180269241333, 0.8032786846160889, 0.7049180269241333, 0.7868852615356445, 0.7540983557701111, 0.7213114500045776, 0.8852459192276001, 0.8360655903816223, 0.8852459192276001]\n"
     ]
    }
   ],
   "source": [
    "print(\"loss =\",history.history[\"loss\"])\n",
    "print(\"val_loss =\",history.history[\"val_loss\"])\n",
    "print(\"acc =\",history.history[\"accuracy\"])\n",
    "print(\"val_acc =\",history.history[\"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (15,) and (11,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14052/3961223150.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ggplot\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train_loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train_acc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2755\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2756\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2757\u001b[1;33m     return gca().plot(\n\u001b[0m\u001b[0;32m   2758\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2759\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1630\u001b[0m         \"\"\"\n\u001b[0;32m   1631\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1632\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1633\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1634\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    499\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (15,) and (11,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAARX0lEQVR4nO3cW2xU5d7H8d+0E21IS9O9Jm1TWjVUjlc0TKxpIlqYNERDbCTBC260acA0yEE0CqJySM3EQFASUAxNMcY7TcQbTDOhkUNVim0JhyAdQojYYu0MZ0TpzHovXvYs5211Taedlnee7yfZyV7MU/vf/735MvvRjse2bVsAgKyXM9kDAAAmBsEHAEMQfAAwBMEHAEMQfAAwBMEHAEN43Q7s2bNHXV1dKiws1I4dO4a9btu2Wltb1d3drYcfflhNTU2aPn16RoYFAKTP9R3+M888o40bN/7j693d3bpy5Yp27dqlFStWaN++feM6IABgfLgGf+7cucrPz//H10+cOKEFCxbI4/Fo5syZun37tq5evTquQwIAxs71SsdNNBqVz+dLPFuWpWg0qqKiomFnQ6GQQqGQJCkYDI71WwMARmHMwR/pkxk8Hs+IZwOBgAKBQOK5r69vrN8+K/h8Pg0ODk72GA8EduFgFw524SgrK0v7a8f8T+lYlpX0X0QkEhnx3T0AYHKNOfh+v1+HDx+Wbds6f/68pkyZQvAB4AHkeqXz4Ycf6uzZs7p586ZeeeUVLVu2TENDQ5Kkuro6VVVVqaurS6tXr9ZDDz2kpqamjA8NABg91+CvXbv2X1/3eDxqbGwcr3kAABnCT9oCgCEIPgAYguADgCEIPgAYguADgCEIPgAYguADgCEIPgAYguADgCEIPgAYguADgCEIPgAYguADgCEIPgAYguADgCEIPgAYguADgCEIPgAYguADgCEIPgAYguADgCEIPgAYguADgCEIPgAYguADgCEIPgAYguADgCEIPgAYguADgCEIPgAYguADgCEIPgAYguADgCEIPgAYguADgCG8qRzq6elRa2ur4vG4Fi1apPr6+qTX79y5o127dikSiSgWi2nJkiWqra3NxLwAgDS5Bj8ej6ulpUWbNm2SZVnasGGD/H6/ysvLE2e+/fZblZeX66233tKNGze0Zs0aPfXUU/J6U/rzBAAwAVyvdMLhsEpLS1VSUiKv16uamhp1dnYmnfF4PLp7965s29bdu3eVn5+vnBxuiwDgQeL6FjwajcqyrMSzZVnq7e1NOrN48WJ98MEHWrlypf744w+tW7duxOCHQiGFQiFJUjAYlM/nG+v8WcHr9bKL+9iFg1042MX4cA2+bdvDfs3j8SQ9nzx5Uo8++qjeffdd/fbbb9q2bZtmz56tKVOmJJ0LBAIKBAKJ58HBwXTnzio+n49d3McuHOzCwS4cZWVlaX+t672LZVmKRCKJ50gkoqKioqQz7e3tqq6ulsfjUWlpqYqLi9XX15f2UACA8eca/MrKSvX392tgYEBDQ0Pq6OiQ3+9POuPz+XTq1ClJ0rVr19TX16fi4uLMTAwASIvrlU5ubq4aGhrU3NyseDyu2tpaVVRUqK2tTZJUV1enpUuXas+ePVq/fr0kafny5Zo6dWpmJwcAjIrHHumSfoJw7fO/uJ90sAsHu3CwC0dG7/ABANmB4AOAIQg+ABiC4AOAIQg+ABiC4AOAIQg+ABiC4AOAIQg+ABiC4AOAIQg+ABiC4AOAIQg+ABiC4AOAIQg+ABiC4AOAIQg+ABiC4AOAIQg+ABiC4AOAIQg+ABiC4AOAIQg+ABiC4AOAIQg+ABiC4AOAIQg+ABiC4AOAIQg+ABiC4AOAIQg+ABiC4AOAIQg+ABiC4AOAIbypHOrp6VFra6vi8bgWLVqk+vr6YWfOnDmj/fv3KxaLqaCgQFu2bBnvWQEAY+Aa/Hg8rpaWFm3atEmWZWnDhg3y+/0qLy9PnLl9+7b27dunt99+Wz6fT9evX8/o0ACA0XO90gmHwyotLVVJSYm8Xq9qamrU2dmZdObo0aOqrq6Wz+eTJBUWFmZmWgBA2lzf4UejUVmWlXi2LEu9vb1JZ/r7+zU0NKTNmzfrjz/+0LPPPqunn3562F8rFAopFApJkoLBYOIPCNN5vV52cR+7cLALB7sYH67Bt2172K95PJ6k51gsposXL+qdd97RX3/9pU2bNmnGjBkqKytLOhcIBBQIBBLPg4OD6c6dVXw+H7u4j1042IWDXTj+b1dHwzX4lmUpEokkniORiIqKioadKSgoUF5envLy8jRnzhxdunRpTIMBAMaX6x1+ZWWl+vv7NTAwoKGhIXV0dMjv9yed8fv9OnfunGKxmP7880+Fw2FNmzYtY0MDAEbP9R1+bm6uGhoa1NzcrHg8rtraWlVUVKitrU2SVFdXp/Lycs2bN0+vv/66cnJytHDhQj3yyCMZHx4AkDqPPdIl/QTp6+ubrG/9QOF+0sEuHOzCwS4cY7kq5ydtAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQKQW/p6dHa9as0auvvqqvv/76H8+Fw2G9+OKL+uGHH8ZrPgDAOHENfjweV0tLizZu3KidO3fq2LFjunz58ojnvvjiC82bNy8TcwIAxsg1+OFwWKWlpSopKZHX61VNTY06OzuHnTt48KCqq6s1derUjAwKABgbr9uBaDQqy7ISz5Zlqbe3d9iZ48eP67333tPHH3/8j3+tUCikUCgkSQoGg/L5fOnOnVW8Xi+7uI9dONiFg12MD9fg27Y97Nc8Hk/S8/79+7V8+XLl5Pz7/2EIBAIKBAKJ58HBwVTnzGo+n49d3McuHOzCwS4cZWVlaX+ta/Aty1IkEkk8RyIRFRUVJZ25cOGCPvroI0nSjRs31N3drZycHD3xxBNpDwYAGF+uwa+srFR/f78GBgb0n//8Rx0dHVq9enXSmd27dyf9+/nz5xN7AHjAuAY/NzdXDQ0Nam5uVjweV21trSoqKtTW1iZJqqury/iQAICx89gjXdJPkL6+vsn61g8U7icd7MLBLhzswjGWO3x+0hYADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQBB8ADEHwAcAQ3lQO9fT0qLW1VfF4XIsWLVJ9fX3S60eOHNGBAwckSXl5eWpsbNRjjz023rMCAMbA9R1+PB5XS0uLNm7cqJ07d+rYsWO6fPly0pni4mJt3rxZ27dv19KlS/Xpp59mbGAAQHpcgx8Oh1VaWqqSkhJ5vV7V1NSos7Mz6cysWbOUn58vSZoxY4YikUhmpgUApM31SicajcqyrMSzZVnq7e39x/OHDh1SVVXViK+FQiGFQiFJUjAYlM/nG+28Wcnr9bKL+9iFg1042MX4cA2+bdvDfs3j8Yx49vTp02pvb9fWrVtHfD0QCCgQCCSeBwcHU50zq/l8PnZxH7twsAsHu3CUlZWl/bWuVzqWZSVd0UQiERUVFQ07d+nSJe3du1dvvPGGCgoK0h4IAJAZrsGvrKxUf3+/BgYGNDQ0pI6ODvn9/qQzg4OD2r59u1atWjWmP30AAJnjeqWTm5urhoYGNTc3Kx6Pq7a2VhUVFWpra5Mk1dXV6csvv9StW7e0b9++xNcEg8HMTg4AGBWPPdIl/QTp6+ubrG/9QOF+0sEuHOzCwS4cGb3DBwBkB4IPAIYg+ABgCIIPAIYg+ABgCIIPAIYg+ABgCIIPAIYg+ABgCIIPAIYg+ABgCIIPAIYg+ABgCIIPAIYg+ABgCIIPAIYg+ABgCIIPAIYg+ABgCIIPAIYg+ABgCIIPAIYg+ABgCIIPAIYg+ABgCIIPAIYg+ABgCIIPAIYg+ABgCIIPAIYg+ABgCIIPAIYg+ABgCIIPAIYg+ABgCG8qh3p6etTa2qp4PK5Fixapvr4+6XXbttXa2qru7m49/PDDampq0vTp0zMxLwAgTa7v8OPxuFpaWrRx40bt3LlTx44d0+XLl5POdHd368qVK9q1a5dWrFihffv2ZWxgAEB6XIMfDodVWlqqkpISeb1e1dTUqLOzM+nMiRMntGDBAnk8Hs2cOVO3b9/W1atXMzY0AGD0XK90otGoLMtKPFuWpd7e3mFnfD5f0ploNKqioqKkc6FQSKFQSJIUDAZVVlY2puGzCbtwsAsHu3Cwi7FzfYdv2/awX/N4PKM+I0mBQEDBYFDBYFBvvfXWaObMauzCwS4c7MLBLhxj2YVr8C3LUiQSSTxHIpFh79wty9Lg4OC/ngEATC7X4FdWVqq/v18DAwMaGhpSR0eH/H5/0hm/36/Dhw/Ltm2dP39eU6ZMIfgA8IBxvcPPzc1VQ0ODmpubFY/HVVtbq4qKCrW1tUmS6urqVFVVpa6uLq1evVoPPfSQmpqaXL9xIBAY+/RZgl042IWDXTjYhWMsu/DYI13AAwCyDj9pCwCGIPgAYIiUPlphLPhYBofbLo4cOaIDBw5IkvLy8tTY2KjHHnts4gedAG67+K9wOKy3335b69at05NPPjmxQ06QVHZx5swZ7d+/X7FYTAUFBdqyZcvEDzoB3HZx584d7dq1S5FIRLFYTEuWLFFtbe3kDJtBe/bsUVdXlwoLC7Vjx45hr6fdTTuDYrGYvWrVKvvKlSv2vXv37Ndff93+5Zdfks789NNPdnNzsx2Px+2ff/7Z3rBhQyZHmjSp7OLcuXP2zZs3bdu27a6uLqN38d9zmzdvtt9//337+++/n4RJMy+VXdy6dcteu3at/fvvv9u2bdvXrl2bjFEzLpVdfPXVV/bnn39u27ZtX79+3X7ppZfse/fuTca4GXXmzBn7woUL9muvvTbi6+l2M6NXOnwsgyOVXcyaNUv5+fmSpBkzZiT9/EM2SWUXknTw4EFVV1dr6tSpkzDlxEhlF0ePHlV1dXXip9kLCwsnY9SMS2UXHo9Hd+/elW3bunv3rvLz85WTk30303Pnzk20YCTpdjOjmxrpYxmi0eiwMyN9LEO2SWUXf3fo0CFVVVVNxGgTLtX/XRw/flx1dXUTPd6ESmUX/f39unXrljZv3qw333xT33333USPOSFS2cXixYv166+/auXKlVq/fr1efvnlrAy+m3S7mdE7fHscP5bh/7vR/Oc8ffq02tvbtXXr1kyPNSlS2cX+/fu1fPnyrP/NnMouYrGYLl68qHfeeUd//fWXNm3apBkzZmTdZ8uksouTJ0/q0Ucf1bvvvqvffvtN27Zt0+zZszVlypSJGvOBkG43Mxp8PpbBkcouJOnSpUvau3evNmzYoIKCgokcccKksosLFy7oo48+kiTduHFD3d3dysnJ0RNPPDGhs2Zaqr9HCgoKlJeXp7y8PM2ZM0eXLl3KuuCnsov29nbV19fL4/GotLRUxcXF6uvr0+OPPz7R406qdLuZ0bdPfCyDI5VdDA4Oavv27Vq1alXW/Wb+u1R2sXv37sS/nnzySTU2NmZd7KXUf4+cO3dOsVhMf/75p8LhsKZNmzZJE2dOKrvw+Xw6deqUJOnatWvq6+tTcXHxZIw7qdLtZsZ/0rarq0ufffZZ4mMZXnjhhaSPZbBtWy0tLTp58mTiYxkqKyszOdKkcdvFJ598oh9//DFxN5ebm6tgMDiZI2eM2y7+bvfu3Zo/f37W/mOZqezim2++UXt7u3JycrRw4UI999xzkzlyxrjtIhqNas+ePYm/Qfn8889rwYIFkzlyRnz44Yc6e/asbt68qcLCQi1btkxDQ0OSxtZNPloBAAyR3X9HDACQQPABwBAEHwAMQfABwBAEHwAMQfABwBAEHwAM8T/BGpEON/eCRQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 15\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "no_mask\n",
      "ffp2\n",
      "ffp2\n",
      "op_mask\n",
      "ffp2\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "ffp2\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "ffp2\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "ffp2\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "ffp2\n",
      "op_mask\n",
      "no_mask\n",
      "op_mask\n",
      "no_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "ffp2\n",
      "op_mask\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "ffp2\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "ffp2\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "ffp2\n",
      "no_mask\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "op_mask\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "no_mask\n",
      "ffp2\n",
      "no_mask\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "ffp2\n",
      "no_mask\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "no_mask\n",
      "ffp2\n",
      "no_mask\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "no_mask\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "no_mask\n",
      "no_mask\n",
      "ffp2\n",
      "ffp2\n",
      "no_mask\n",
      "no_mask\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "no_mask\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "no_mask\n",
      "ffp2\n",
      "ffp2\n",
      "op_mask\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "ffp2\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "ffp2\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "ffp2\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n",
      "op_mask\n"
     ]
    }
   ],
   "source": [
    "img_path='../img/ffp2/IMG_1599.JPG_(350.0, 92.0, 1009.0, 599.0).png'\n",
    "img_path='../img/no_mask/Foto 22.01.22, 19 02 46.jpg_(103.0, 94.0, 619.0, 848.0).png'\n",
    "#img_path='../img/op_mask/IMG_1337.JPG_(878, 710, 954, 954).png'\n",
    "#img_path='../img/other_mask/IMG_1327.JPG_(863, 476, 1097, 1097).png'\n",
    "#predict('probabilities',img_path=img_path,  model=model, labels=labels, img_size=(180,180))\n",
    "\n",
    "predict('live_detection', model=model, labels=labels, img_size=(180,180))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "320b063615c881cf99d5bda0a05a87a093cceae5b3aed84278fa7ef3d4b95f2a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
