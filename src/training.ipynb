{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing opencv haarcascade face detection network\n",
    "https://github.com/opencv/opencv/tree/master/data/haarcascades\n",
    "explained in: https://www.youtube.com/watch?v=7IFhsbfby9s&t=300s (or gitHub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertImg(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def faceNetLocalize(img):\n",
    "    img_cvt = convertImg(img)\n",
    "    return faceNet.detectMultiScale3(img_cvt, scaleFactor=scaleFactor, minNeighbors=minNeighbors, minSize=minSize, outputRejectLevels = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaleFactor = 1.1 #between 1.05 (quality) and 1.4 (speed) recommended (scale of the faces we search for)\n",
    "minNeighbors = 4 #between 3 (quantity) and 6 (quality) recommended\n",
    "minSize = (10, 10) #min size of a face in the picture\n",
    "#maxSize = (240, 240) #max face size in picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: testing with different model types\n",
    "#for example: eye model + larger bounding box towards the bottom\n",
    "faceNet = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing - Face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO can we even use accuracy methods? should we just test freely with live feed and empirical testing?\n",
    "#labels_path = os.path.join('..', 'img', 'test', 'labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask classifier\n",
    "\n",
    "foundation: https://www.tensorflow.org/tutorials/load_data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import MaxPool2D, Conv2D, Input, Dense, Flatten, AveragePooling2D, Dropout\n",
    "import tensorflow.keras.layers as lays\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling, RandomContrast, RandomFlip, RandomRotation\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras import Sequential, losses as lfs\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation\n",
    "https://www.tensorflow.org/tutorials/images/data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = Sequential([\n",
    "  RandomFlip(\"horizontal\"),\n",
    "  RandomRotation(0.4),\n",
    "  RandomContrast(0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_size = (180, 180)\n",
    "img_size_vgg = (224, 224)\n",
    "epochs = 20\n",
    "checkpoint_path = \"mask_model/weights.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "imgs_path = os.path.join('..', 'img')\n",
    "num_classes = 2\n",
    "\n",
    "correct_usage=  'correct usage: \\n' + 'predict([path to image], \\'category\\' \\n' + 'predict([path to image], \\'probabilities\\' \\n' + 'predict([path to image], \\'detection\\' \\n' + 'predict(\\'live_detection\\')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loading dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 293 files belonging to 2 classes.\n",
      "Using 235 files for training.\n",
      "Found 293 files belonging to 2 classes.\n",
      "Using 58 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = image_dataset_from_directory(imgs_path,  validation_split=0.2, subset=\"training\",  seed=3, image_size=img_size,  batch_size=batch_size)\n",
    "val_ds = image_dataset_from_directory(imgs_path,  validation_split=0.2, subset=\"validation\",  seed=3, image_size=img_size,  batch_size=batch_size)\n",
    "labels = train_ds.class_names\n",
    "\n",
    "y_test = np.concatenate([y for _, y in val_ds], axis=0)\n",
    "x_test = np.concatenate([x for x, _ in val_ds], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 31 Complete [00h 00m 28s]\n",
      "accuracy: 0.5862069129943848\n",
      "\n",
      "Best accuracy So Far: 0.7068965435028076\n",
      "Total elapsed time: 00h 02m 06s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "<keras_tuner.engine.hyperparameters.HyperParameters object at 0x000002030AFECC40>\n"
     ]
    }
   ],
   "source": [
    "#Hypermodels\n",
    "#https://www.analyticsvidhya.com/blog/2021/06/create-convolutional-neural-network-model-and-optimize-using-keras-tuner-deep-learning/\n",
    "#https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    kt.applications.HyperResNet(input_shape=(180, 180, 3), classes=2),\n",
    "    objective='val_loss',\n",
    "    max_trials=5\n",
    ")\n",
    "\n",
    "def basic_model_builder(hp):\n",
    "  \n",
    "    basic_model = Sequential([\n",
    "        Rescaling(1. /255),\n",
    "        augmentation,\n",
    "        Conv2D(filters=hp.Int('c1_filter', min_value=32, max_value=256, step=16), kernel_size=hp.Choice('c1_kernel', values=[3,5]), activation='relu'),\n",
    "        AveragePooling2D(pool_size=(7,7)),\n",
    "        Flatten(name=\"flatten\"),\n",
    "        Dense(units=hp.Int('d1_units', min_value=32, max_value=512, step=32), activation=\"relu\"),\n",
    "        Dropout(0.5),#drops small confidences\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    basic_model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), loss=lfs.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    return basic_model\n",
    "\n",
    "#tuner = kt.RandomSearch(basic_model_builder, objective='val_accuracy', max_trials=5)\n",
    "tuner = kt.Hyperband(basic_model_builder, objective='accuracy', max_epochs=10, factor=3)\n",
    "tuner.search(x_test, y_test, epochs=50, validation_split=0)\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "basic_model = Sequential([ \n",
    "    Rescaling(1. /255),\n",
    "    augmentation,\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    AveragePooling2D(pool_size=(7,7)),\n",
    "    Flatten(name=\"flatten\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.5),#drops small confidences\n",
    "    Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "vgg_small_model = Sequential([ \n",
    "    Rescaling(1. /255),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Flatten(name=\"flatten\"),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dropout(0.5),#drops small confidences\n",
    "    Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "vgg_model = Sequential([\n",
    "    Rescaling(1. /255),\n",
    "    Conv2D(input_shape=(224,224,3), filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", strides=(1,1)), \n",
    "    Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2)),\n",
    "    Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2)),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2)),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2)),\n",
    "    Flatten(),\n",
    "    Dense(units=4096, activation=\"relu\"),\n",
    "    Dense(units=4096, activation=\"relu\"),\n",
    "    Dense(units=4, activation=\"softmax\")\n",
    "])\n",
    "model = basic_model\n",
    "#\n",
    "model.compile(optimizer='adam', loss=lfs.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about the VGG-model: https://neurohive.io/en/popular-networks/vgg16/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8/8 [==============================] - 13s 345ms/step - loss: 1.5170 - accuracy: 0.5532 - val_loss: 0.6432 - val_accuracy: 0.5517\n",
      "\n",
      "Epoch 00001: saving model to mask_model\\weights.ckpt\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 3s 265ms/step - loss: 0.7100 - accuracy: 0.5787 - val_loss: 0.6190 - val_accuracy: 0.8448\n",
      "\n",
      "Epoch 00002: saving model to mask_model\\weights.ckpt\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 3s 253ms/step - loss: 0.5966 - accuracy: 0.6979 - val_loss: 0.5116 - val_accuracy: 0.6552\n",
      "\n",
      "Epoch 00003: saving model to mask_model\\weights.ckpt\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 3s 273ms/step - loss: 0.4850 - accuracy: 0.8298 - val_loss: 0.4434 - val_accuracy: 0.7069\n",
      "\n",
      "Epoch 00004: saving model to mask_model\\weights.ckpt\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 3s 247ms/step - loss: 0.4716 - accuracy: 0.7574 - val_loss: 0.3125 - val_accuracy: 0.9483\n",
      "\n",
      "Epoch 00005: saving model to mask_model\\weights.ckpt\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 3s 276ms/step - loss: 0.3414 - accuracy: 0.8723 - val_loss: 0.2833 - val_accuracy: 0.9483\n",
      "\n",
      "Epoch 00006: saving model to mask_model\\weights.ckpt\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 4s 326ms/step - loss: 0.3299 - accuracy: 0.8766 - val_loss: 0.2561 - val_accuracy: 0.9483\n",
      "\n",
      "Epoch 00007: saving model to mask_model\\weights.ckpt\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 4s 321ms/step - loss: 0.2730 - accuracy: 0.9064 - val_loss: 0.2338 - val_accuracy: 0.9483\n",
      "\n",
      "Epoch 00008: saving model to mask_model\\weights.ckpt\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 4s 328ms/step - loss: 0.2488 - accuracy: 0.9191 - val_loss: 0.2272 - val_accuracy: 0.9483\n",
      "\n",
      "Epoch 00009: saving model to mask_model\\weights.ckpt\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 8s 298ms/step - loss: 0.2138 - accuracy: 0.9191 - val_loss: 0.2338 - val_accuracy: 0.9310\n",
      "\n",
      "Epoch 00010: saving model to mask_model\\weights.ckpt\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 3s 261ms/step - loss: 0.2321 - accuracy: 0.9106 - val_loss: 0.2039 - val_accuracy: 0.9483\n",
      "\n",
      "Epoch 00011: saving model to mask_model\\weights.ckpt\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 3s 271ms/step - loss: 0.2091 - accuracy: 0.9191 - val_loss: 0.2243 - val_accuracy: 0.9483\n",
      "\n",
      "Epoch 00012: saving model to mask_model\\weights.ckpt\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 3s 255ms/step - loss: 0.2509 - accuracy: 0.9106 - val_loss: 0.3257 - val_accuracy: 0.8793\n",
      "\n",
      "Epoch 00013: saving model to mask_model\\weights.ckpt\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 3s 299ms/step - loss: 0.3442 - accuracy: 0.8553 - val_loss: 0.2006 - val_accuracy: 0.9655\n",
      "\n",
      "Epoch 00014: saving model to mask_model\\weights.ckpt\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 4s 275ms/step - loss: 0.2694 - accuracy: 0.9149 - val_loss: 0.2054 - val_accuracy: 0.9655\n",
      "\n",
      "Epoch 00015: saving model to mask_model\\weights.ckpt\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 4s 294ms/step - loss: 0.2366 - accuracy: 0.9362 - val_loss: 0.2490 - val_accuracy: 0.8966\n",
      "\n",
      "Epoch 00016: saving model to mask_model\\weights.ckpt\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 8s 279ms/step - loss: 0.2118 - accuracy: 0.9234 - val_loss: 0.1955 - val_accuracy: 0.9655\n",
      "\n",
      "Epoch 00017: saving model to mask_model\\weights.ckpt\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 4s 268ms/step - loss: 0.2290 - accuracy: 0.9404 - val_loss: 0.2343 - val_accuracy: 0.9138\n",
      "\n",
      "Epoch 00018: saving model to mask_model\\weights.ckpt\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 3s 266ms/step - loss: 0.1993 - accuracy: 0.9234 - val_loss: 0.2037 - val_accuracy: 0.9483\n",
      "\n",
      "Epoch 00019: saving model to mask_model\\weights.ckpt\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 3s 296ms/step - loss: 0.2213 - accuracy: 0.9362 - val_loss: 0.2078 - val_accuracy: 0.9483\n",
      "\n",
      "Epoch 00020: saving model to mask_model\\weights.ckpt\n"
     ]
    }
   ],
   "source": [
    "#Callback to save model's weights\n",
    "#https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose = 1)\n",
    "\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation (mask):\n",
    "https://www.tensorflow.org/guide/keras/train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 40ms/step - loss: 2.5196 - accuracy: 0.4310\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.50      0.49        32\n",
      "           1       0.36      0.35      0.35        26\n",
      "\n",
      "    accuracy                           0.43        58\n",
      "   macro avg       0.42      0.42      0.42        58\n",
      "weighted avg       0.43      0.43      0.43        58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred_confidences = model.predict(x_test)\n",
    "y_pred = [np.argmax(cs) for cs in y_pred_confidences]\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskPredict(img):\n",
    "    pred = model.predict(img[None])\n",
    "    #TODO: richtiges mapping zwischen index/label?\n",
    "    label_index = np.argmax(pred)\n",
    "    return labels[label_index], pred[0][label_index]\n",
    "\n",
    "\n",
    "#mode can be 'category', 'probabilities', 'detection', 'live_detection'\n",
    "def predict(img_path, mode):\n",
    "    #load_img\n",
    "    img = load_img(img_path, target_size = img_size)\n",
    "    img = img_to_array(img)\n",
    "    \n",
    "    if mode=='detection':\n",
    "        detect(img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        return\n",
    "    if mode=='live_detection':\n",
    "        live_det()\n",
    "        return\n",
    "        \n",
    "    if mode=='category':\n",
    "        label, confidence = maskPredict(img)\n",
    "        return label\n",
    "        \n",
    "    if mode=='probabilities':\n",
    "        #TODO: sch√∂neres Format\n",
    "        return model.predict(img[None])\n",
    "\n",
    "    else:\n",
    "        print(correct_usage)\n",
    "\n",
    "#def predict(mode):\n",
    "#    if mode=='live_detection':\n",
    "#        predict('', mode)\n",
    "#    else:\n",
    "#        print(correct_usage)\n",
    "\n",
    "def detect(img):\n",
    "    faceLocs, rejectLevels, confidences = faceNetLocalize(img)\n",
    "        \n",
    "    for (x, y, w, h) in faceLocs:\n",
    "        #crop image and predict label of cropped image\n",
    "        img_crop = img[y:y+h, x:x+w]\n",
    "        label, confidence_mask = maskPredict(img)\n",
    "        #show label/ bounding box on image\n",
    "        cv2.putText(img, f\"{label}, confidence:{confidence_mask}\", (x+w-30, y+h), cv2.FONT_HERSHEY_PLAIN, 1.0, cv2.CV_RGB(0,255,0), 2.0) \n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow('live_output', img)\n",
    "\n",
    "def live_det():\n",
    "    #TODO: errorhandling for camera\n",
    "    \n",
    "    wait_time = 10 #time in ms to wait before refreshing feed\n",
    "    camera = cv2.VideoCapture(0) #Input value might differ on different systems\n",
    "    \n",
    "    while(True):\n",
    "        _, img = camera.read()\n",
    "\n",
    "        detect(img)\n",
    "\n",
    "        #wait for ESC or q\n",
    "        if (cv2.waitKey(wait_time) & 0xFF) in [27, ord('q')]: \n",
    "            break\n",
    "\n",
    "\n",
    "    camera.release()\n",
    "    return 'live_output'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path='../img/ffp2/IMG_1596_(657, 421, 1426, 1426).png'\n",
    "img_path='../img/no_mask/IMG_1323.JPG_(1025, 427, 984, 984).png'\n",
    "img_path='../img/op_mask/IMG_1337.JPG_(878, 710, 954, 954).png'\n",
    "#img_path='../img/other_mask/IMG_1327.JPG_(863, 476, 1097, 1097).png'\n",
    "\n",
    "#predict(img_path, 'live_detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "320b063615c881cf99d5bda0a05a87a093cceae5b3aed84278fa7ef3d4b95f2a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
