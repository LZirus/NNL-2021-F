{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing opencv haarcascade face detection network\n",
    "https://github.com/opencv/opencv/tree/master/data/haarcascades\n",
    "explained in: https://www.youtube.com/watch?v=7IFhsbfby9s&t=300s (or gitHub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertImg(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def faceNetLocalize(img, **kwargs):\n",
    "    scaleFactor = kwargs.get('scaleFactor', 1.1) #between 1.05 (quality) and 1.4 (speed) recommended (scale of the faces we search for)\n",
    "    minNeighbors = kwargs.get('minNeighbors', 4) #between 3 (quantity) and 6 (quality) recommended\n",
    "    minSize = kwargs.get('minSize', (10, 10)) #min size of a face in the picture\n",
    "    faceNet = kwargs.get('faceNet', init_faceNet())\n",
    "    \n",
    "    img_cvt = convertImg(img)\n",
    "    return faceNet.detectMultiScale3(img_cvt, scaleFactor=scaleFactor, minNeighbors=minNeighbors, minSize=minSize, outputRejectLevels = True)\n",
    "\n",
    "def init_faceNet(**kwargs):\n",
    "    path = kwargs.get('path', 'haarcascade_frontalface_default.xml')\n",
    "    return cv2.CascadeClassifier(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: testing with different model types\n",
    "#for example: eye model + larger bounding box towards the bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask classifier\n",
    "\n",
    "foundation: https://www.tensorflow.org/tutorials/load_data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import MaxPool2D, Conv2D, Input, Dense, Flatten, AveragePooling2D, Dropout\n",
    "import tensorflow.keras.layers as lays\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling, RandomContrast, RandomFlip, RandomRotation\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras import Sequential, losses as lfs\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation\n",
    "https://www.tensorflow.org/tutorials/images/data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = Sequential([\n",
    "  RandomFlip(\"horizontal\"),\n",
    "  RandomRotation(0.4),\n",
    "  RandomContrast(0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_size = (180, 180)\n",
    "img_size_vgg = (224, 224)\n",
    "epochs = 11\n",
    "checkpoint_path = \"mask_model/weights.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "imgs_path = os.path.join('..', 'img')\n",
    "num_classes = 2\n",
    "\n",
    "correct_usage=  'correct usage: \\n' + 'predict([path to image], \\'category\\' \\n' + 'predict([path to image], \\'probabilities\\' \\n' + 'predict([path to image], \\'detection\\' \\n' + 'predict(\\'live_detection\\')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loading dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 293 files belonging to 2 classes.\n",
      "Using 235 files for training.\n",
      "Found 293 files belonging to 2 classes.\n",
      "Using 58 files for validation.\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(**kwargs):\n",
    "    imgs_path = kwargs.get('imgs_path', os.path.join('..', 'img'))\n",
    "    img_size = kwargs.get('img_size', (180, 180))\n",
    "    batch_size = kwargs.get('batch_size', 32)\n",
    "\n",
    "    train_ds = image_dataset_from_directory(imgs_path,  validation_split=0.2, subset=\"training\",  seed=3, image_size=img_size,  batch_size=batch_size)\n",
    "    val_ds = image_dataset_from_directory(imgs_path,  validation_split=0.2, subset=\"validation\",  seed=3, image_size=img_size,  batch_size=batch_size)\n",
    "    labels = train_ds.class_names\n",
    "\n",
    "    y_test = np.concatenate([y for _, y in val_ds], axis=0)\n",
    "    x_test = np.concatenate([x for x, _ in val_ds], axis=0)\n",
    "    return train_ds, val_ds, labels, y_test, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 Complete [00h 05m 22s]\n",
      "accuracy: 0.568965494632721\n",
      "\n",
      "Best accuracy So Far: 0.6379310488700867\n",
      "Total elapsed time: 00h 58m 19s\n",
      "\n",
      "Search: Running Trial #10\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "c1_filter         |48                |128               \n",
      "c1_kernel         |5                 |5                 \n",
      "c2_filter         |240               |128               \n",
      "c2_kernel         |5                 |5                 \n",
      "d1_units          |480               |224               \n",
      "learning_rate     |0.001             |0.01              \n",
      "tuner/epochs      |2                 |2                 \n",
      "tuner/initial_e...|0                 |0                 \n",
      "tuner/bracket     |2                 |2                 \n",
      "tuner/round       |0                 |0                 \n",
      "\n",
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "#Hypermodels\n",
    "#https://www.analyticsvidhya.com/blog/2021/06/create-convolutional-neural-network-model-and-optimize-using-keras-tuner-deep-learning/\n",
    "#https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "\n",
    "def basic_model_builder(hp):\n",
    "  \n",
    "    basic_model = Sequential([\n",
    "        Rescaling(1. /255),\n",
    "        augmentation,\n",
    "        Conv2D(filters=hp.Int('c1_filter', min_value=32, max_value=256, step=16), kernel_size=hp.Choice('c1_kernel', values=[3,5]), activation='relu'),\n",
    "        AveragePooling2D(pool_size=(7,7)),\n",
    "        Flatten(name=\"flatten\"),\n",
    "        Dense(units=hp.Int('d1_units', min_value=32, max_value=512, step=32), activation=\"relu\"),\n",
    "        Dropout(0.5),#drops small confidences\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    basic_model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), loss=lfs.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    return basic_model\n",
    "\n",
    "def small_model_builder(hp):\n",
    "  \n",
    "    model = Sequential([\n",
    "        Rescaling(1. /255),\n",
    "        augmentation,\n",
    "        Conv2D(filters=hp.Int('c1_filter', min_value=32, max_value=256, step=16), kernel_size=hp.Choice('c1_kernel', values=[3,5]), activation='relu'),\n",
    "        Conv2D(filters=hp.Int('c2_filter', min_value=32, max_value=256, step=16), kernel_size=hp.Choice('c2_kernel', values=[3,5]), activation='relu'),\n",
    "        MaxPool2D(pool_size=(3,3)),\n",
    "        Flatten(name=\"flatten\"),\n",
    "        Dense(units=hp.Int('d1_units', min_value=32, max_value=512, step=32), activation=\"relu\"),\n",
    "        Dropout(0.5),#drops small confidences\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), loss=lfs.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def tune_model(model_builder, xs, ys):\n",
    "    #tuner = kt.RandomSearch(model_builder, objective='val_accuracy', max_trials=5)\n",
    "    #tuner = kt.Hyperband(model_builder, objective='accuracy', max_epochs=10, factor=3)\n",
    "    tuner = kt.RandomSearch(kt.applications.HyperResNet(input_shape=(180, 180, 3), classes=2), objective='val_loss', max_trials=5)\n",
    "    tuner.search(xs, ys, epochs=50, validation_split=0.2)\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = Sequential([ \n",
    "    Rescaling(1. /255),\n",
    "    augmentation,\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    AveragePooling2D(pool_size=(7,7)),\n",
    "    Flatten(name=\"flatten\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.5),#drops small confidences\n",
    "    Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "small_model = Sequential([ \n",
    "    Rescaling(1. /255),\n",
    "    augmentation,\n",
    "    Conv2D(filters=128, kernel_size=(5,5), activation='relu'),\n",
    "    Conv2D(filters=128, kernel_size=(5,5), activation='relu'),\n",
    "    MaxPool2D(pool_size=(3,3)),\n",
    "    Flatten(name=\"flatten\"),\n",
    "    Dense(units=224, activation=\"relu\"),\n",
    "    Dropout(0.5),#drops small confidences\n",
    "    Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "vgg_small_model = Sequential([ \n",
    "    Rescaling(1. /255),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Flatten(name=\"flatten\"),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dropout(0.5),#drops small confidences\n",
    "    Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "vgg_model = Sequential([\n",
    "    Rescaling(1. /255),\n",
    "    Conv2D(input_shape=(224,224,3), filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", strides=(1,1)), \n",
    "    Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2)),\n",
    "    Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2)),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2)),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2)),\n",
    "    Flatten(),\n",
    "    Dense(units=4096, activation=\"relu\"),\n",
    "    Dense(units=4096, activation=\"relu\"),\n",
    "    Dense(units=4, activation=\"softmax\")\n",
    "])\n",
    "model = small_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about the VGG-model: https://neurohive.io/en/popular-networks/vgg16/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4942: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 174s 20s/step - loss: 2722.5886 - accuracy: 0.5915 - val_loss: 0.6939 - val_accuracy: 0.4483\n",
      "\n",
      "Epoch 00001: saving model to mask_model\\weights.ckpt\n",
      "Epoch 2/11\n",
      "8/8 [==============================] - 131s 16s/step - loss: 1.1520 - accuracy: 0.5362 - val_loss: 0.6945 - val_accuracy: 0.4483\n",
      "\n",
      "Epoch 00002: saving model to mask_model\\weights.ckpt\n",
      "Epoch 3/11\n",
      "8/8 [==============================] - 117s 14s/step - loss: 2.4570 - accuracy: 0.5106 - val_loss: 0.7862 - val_accuracy: 0.4655\n",
      "\n",
      "Epoch 00003: saving model to mask_model\\weights.ckpt\n",
      "Epoch 4/11\n",
      "8/8 [==============================] - 114s 13s/step - loss: 0.8825 - accuracy: 0.5319 - val_loss: 0.6992 - val_accuracy: 0.4483\n",
      "\n",
      "Epoch 00004: saving model to mask_model\\weights.ckpt\n",
      "Epoch 5/11\n",
      "8/8 [==============================] - 115s 14s/step - loss: 0.7020 - accuracy: 0.5149 - val_loss: 0.6965 - val_accuracy: 0.4483\n",
      "\n",
      "Epoch 00005: saving model to mask_model\\weights.ckpt\n",
      "Epoch 6/11\n",
      "8/8 [==============================] - 118s 14s/step - loss: 0.6945 - accuracy: 0.4681 - val_loss: 0.6935 - val_accuracy: 0.4483\n",
      "\n",
      "Epoch 00006: saving model to mask_model\\weights.ckpt\n",
      "Epoch 7/11\n",
      "8/8 [==============================] - 119s 14s/step - loss: 0.6929 - accuracy: 0.5404 - val_loss: 0.6915 - val_accuracy: 0.5517\n",
      "\n",
      "Epoch 00007: saving model to mask_model\\weights.ckpt\n",
      "Epoch 8/11\n",
      "8/8 [==============================] - 118s 14s/step - loss: 0.6920 - accuracy: 0.5319 - val_loss: 0.6902 - val_accuracy: 0.5517\n",
      "\n",
      "Epoch 00008: saving model to mask_model\\weights.ckpt\n",
      "Epoch 9/11\n",
      "8/8 [==============================] - 109s 13s/step - loss: 0.6911 - accuracy: 0.5319 - val_loss: 0.6892 - val_accuracy: 0.5517\n",
      "\n",
      "Epoch 00009: saving model to mask_model\\weights.ckpt\n",
      "Epoch 10/11\n",
      "8/8 [==============================] - 109s 13s/step - loss: 0.6928 - accuracy: 0.5319 - val_loss: 0.6891 - val_accuracy: 0.5517\n",
      "\n",
      "Epoch 00010: saving model to mask_model\\weights.ckpt\n",
      "Epoch 11/11\n",
      "8/8 [==============================] - 110s 13s/step - loss: 0.6903 - accuracy: 0.5319 - val_loss: 0.6889 - val_accuracy: 0.5517\n",
      "\n",
      "Epoch 00011: saving model to mask_model\\weights.ckpt\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_ds, val_ds, **kwargs):\n",
    "    epochs = kwargs.get('epochs', 10)\n",
    "    checkpoint_path = kwargs.get('checkpoint_path', checkpoint_path = \"mask_model/weights.ckpt\")\n",
    "\n",
    "    #Callback to save model's weights\n",
    "    #https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "    cp_callback = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose = 1)\n",
    "\n",
    "    model.compile(optimizer=Adam(0.01), loss=lfs.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=[cp_callback])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation (mask):\n",
    "https://www.tensorflow.org/guide/keras/train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71        32\n",
      "           1       0.00      0.00      0.00        26\n",
      "\n",
      "    accuracy                           0.55        58\n",
      "   macro avg       0.28      0.50      0.36        58\n",
      "weighted avg       0.30      0.55      0.39        58\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(x_test, y_test, model):\n",
    "    results = model.evaluate(x_test, y_test, batch_size=32)\n",
    "    print(results)\n",
    "    \n",
    "    y_pred_confidences = model.predict(x_test)\n",
    "    y_pred = [np.argmax(cs) for cs in y_pred_confidences]\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path, model):\n",
    "    model.load_weights(checkpoint_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskPredict(img, labels):\n",
    "    pred = model.predict(img[None])\n",
    "    #TODO: richtiges mapping zwischen index/label?\n",
    "    label_index = np.argmax(pred)\n",
    "    return labels[label_index], pred[0][label_index]\n",
    "\n",
    "\n",
    "#mode can be 'category', 'probabilities', 'detection', 'live_detection'\n",
    "def predict(img_path, mode):\n",
    "    #load_img\n",
    "    img = load_img(img_path, target_size = img_size)\n",
    "    img = img_to_array(img)\n",
    "    \n",
    "    if mode=='detection':\n",
    "        detect(img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        return\n",
    "    if mode=='live_detection':\n",
    "        live_det()\n",
    "        return\n",
    "        \n",
    "    if mode=='category':\n",
    "        label, confidence = maskPredict(img)\n",
    "        return label\n",
    "        \n",
    "    if mode=='probabilities':\n",
    "        #TODO: schöneres Format\n",
    "        return model.predict(img[None])\n",
    "\n",
    "    else:\n",
    "        print(correct_usage)\n",
    "\n",
    "#def predict(mode):\n",
    "#    if mode=='live_detection':\n",
    "#        predict('', mode)\n",
    "#    else:\n",
    "#        print(correct_usage)\n",
    "\n",
    "def detect(img):\n",
    "    faceLocs, rejectLevels, confidences = faceNetLocalize(img)\n",
    "        \n",
    "    for (x, y, w, h) in faceLocs:\n",
    "        #crop image and predict label of cropped image\n",
    "        img_crop = img[y:y+h, x:x+w]\n",
    "        label, confidence_mask = maskPredict(img)\n",
    "        #show label/ bounding box on image\n",
    "        cv2.putText(img, f\"{label}, confidence:{confidence_mask}\", (x+w-30, y+h), cv2.FONT_HERSHEY_PLAIN, 1.0, cv2.CV_RGB(0,255,0), 2.0) \n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow('live_output', img)\n",
    "\n",
    "def live_det():\n",
    "    #TODO: errorhandling for camera\n",
    "    \n",
    "    wait_time = 10 #time in ms to wait before refreshing feed\n",
    "    camera = cv2.VideoCapture(0) #Input value might differ on different systems\n",
    "    \n",
    "    while(True):\n",
    "        _, img = camera.read()\n",
    "\n",
    "        detect(img)\n",
    "\n",
    "        #wait for ESC or q\n",
    "        if (cv2.waitKey(wait_time) & 0xFF) in [27, ord('q')]: \n",
    "            break\n",
    "\n",
    "\n",
    "    camera.release()\n",
    "    return 'live_output'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_path='../img/ffp2/IMG_1596_(657, 421, 1426, 1426).png'\n",
    "#img_path='../img/no_mask/IMG_1323.JPG_(1025, 427, 984, 984).png'\n",
    "#img_path='../img/op_mask/IMG_1337.JPG_(878, 710, 954, 954).png'\n",
    "#img_path='../img/other_mask/IMG_1327.JPG_(863, 476, 1097, 1097).png'\n",
    "\n",
    "#predict(img_path, 'live_detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "320b063615c881cf99d5bda0a05a87a093cceae5b3aed84278fa7ef3d4b95f2a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
